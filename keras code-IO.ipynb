{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from keras import regularizers\n",
    "from keras.layers import Bidirectional, Dense, Dropout, Embedding, LSTM, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "from data_preprocessing import open_data, tokenize, tag_document, data_to_seq, glove_emb_matrix\n",
    "from data_preprocessing import tags_to_3D, tags_to_2D, clean_data\n",
    "from validation import precision, recall, f1, retrive_phrase_IO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/valeriya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "documents = {}\n",
    "labels = {}\n",
    "\n",
    "#directories\n",
    "dir_Tu= \"/Users/kmirai/Downloads/NLPProject-master/Hulth2003/Training\"\n",
    "\n",
    "dir_valeria_train = \"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training\"\n",
    "dir_valeria_val = \"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Validation\"\n",
    "dir_valeria_test = \"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Test\"\n",
    "\n",
    "dir_anna = \"/Users/annasotnikova/Downloads/Hulth2003/Training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#open data\n",
    "documents_train, labels_train = open_data(dir_valeria_train)\n",
    "documents_val, labels_val = open_data(dir_valeria_val)\n",
    "documents_test, labels_test = open_data(dir_valeria_test)\n",
    "\n",
    "# tokenize data\n",
    "tokenized_documents_train, tokenized_labels_train = tokenize(documents_train, labels_train)\n",
    "tokenized_documents_val, tokenized_labels_val = tokenize(documents_val, labels_val)\n",
    "tokenized_documents_test, tokenized_labels_test = tokenize(documents_test, labels_test)\n",
    "\n",
    "# create sequence of labels (tags) for the documents\n",
    "tags_train, tokenized_labels_train = tag_document(tokenized_documents_train, tokenized_labels_train)\n",
    "tags_val, tokenized_labels_val = tag_document(tokenized_documents_val, tokenized_labels_val)\n",
    "tags_test, tokenized_labels_test = tag_document(tokenized_documents_test, tokenized_labels_test)\n",
    "\n",
    "# remove documents without keyphrases \n",
    "tokenized_documents_train, tags_train, tokenized_labels_train = clean_data(tokenized_documents_train,\n",
    "                                                                           tags_train, tokenized_labels_train)\n",
    "tokenized_documents_val, tags_val, tokenized_labels_val = clean_data(tokenized_documents_val,\n",
    "                                                                           tags_val, tokenized_labels_val)\n",
    "tokenized_documents_test, tags_test, tokenized_labels_test = clean_data(tokenized_documents_test,\n",
    "                                                                           tags_test, tokenized_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "json = json.dumps(tokenized_labels_test)\n",
    "f = open(\"tokenized_labels_test.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BIO_to_IO(labels):\n",
    "    for key in labels.keys():\n",
    "        for i, token in enumerate(labels[key]):\n",
    "            if token==2:\n",
    "                labels[key][i] = 1\n",
    "    return labels \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags_train = BIO_to_IO(tags_train)\n",
    "tags_val = BIO_to_IO(tags_val)\n",
    "tags_test = BIO_to_IO(tags_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOVE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove = dict()\n",
    "embed_size = 200\n",
    "f = open('glove.6B/glove.6B.200d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove[word] = coefs\n",
    "f.close()\n",
    "\n",
    "glove_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create vocabulary from all data \n",
    "X_train_eng = [doc for doc in copy.deepcopy(tokenized_documents_train).values()]\n",
    "X_val_eng = [doc for doc in copy.deepcopy(tokenized_documents_val).values()]\n",
    "X_test_eng = [doc for doc in copy.deepcopy(tokenized_documents_test).values()]\n",
    "X_full = X_train_eng + X_val_eng + X_test_eng\n",
    "\n",
    "# Our vocab: all the words in all abstracts\n",
    "target_vocab = list(set([token for doc in X_full for token in doc]))\n",
    "# Dictionary with all words and their indices\n",
    "vocab_ind_dict = dict(zip(target_vocab, range(0, len(target_vocab)))) \n",
    "# Embedding matrix\n",
    "embed_matrix = glove_emb_matrix(vocab_ind_dict, glove, glove_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for network\n",
    "X_train = data_to_seq(X_train_eng, vocab_ind_dict)\n",
    "X_val = data_to_seq(X_val_eng, vocab_ind_dict)\n",
    "X_test = data_to_seq(X_test_eng, vocab_ind_dict)\n",
    "\n",
    "kp_train = [doc for doc in copy.deepcopy(tokenized_labels_train).values()]\n",
    "tags_train = [doc for doc in copy.deepcopy(tags_train).values()]\n",
    "kp_val = [doc for doc in copy.deepcopy(tokenized_labels_val).values()]\n",
    "tags_val = [doc for doc in copy.deepcopy(tags_val).values()]\n",
    "kp_test = [doc for doc in copy.deepcopy(tokenized_labels_test).values()]\n",
    "tags_test = [doc for doc in copy.deepcopy(tags_test).values()]\n",
    "\n",
    "# Padding \n",
    "X_train_padded = pad_sequences(X_train, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "X_val_padded = pad_sequences(X_val, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "X_test_padded = pad_sequences(X_test, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "\n",
    "tags_train_padded = pad_sequences(tags_train, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "tags_val_padded = pad_sequences(tags_val, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "tags_test_padded = pad_sequences(tags_test, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "\n",
    "# Convert labels to 3D as keras likes\n",
    "tags_train_2d = tags_to_2D(tags_train_padded)\n",
    "tags_val_2d = tags_to_2D(tags_val_padded)\n",
    "tags_test_2d = tags_to_2D(tags_test_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = np.reshape(class_weight.compute_sample_weight('balanced', tags_train_padded.flatten()),\n",
    "                             np.shape(tags_train_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 550, 200)          3642600   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 550, 600)          1202400   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 550, 600)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 550, 150)          90150     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 550, 150)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 550, 2)            302       \n",
      "=================================================================\n",
      "Total params: 4,935,452\n",
      "Trainable params: 1,292,852\n",
      "Non-trainable params: 3,642,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 993 samples, validate on 492 samples\n",
      "Epoch 1/8\n",
      "993/993 [==============================] - 239s 241ms/step - loss: 0.4648 - accuracy: 0.8857 - val_loss: 0.2681 - val_accuracy: 0.9043\n",
      "Epoch 2/8\n",
      "993/993 [==============================] - 236s 238ms/step - loss: 0.1691 - accuracy: 0.9182 - val_loss: 0.3364 - val_accuracy: 0.8700\n",
      "Epoch 3/8\n",
      "993/993 [==============================] - 236s 238ms/step - loss: 0.1565 - accuracy: 0.9242 - val_loss: 0.1588 - val_accuracy: 0.9365\n",
      "Epoch 4/8\n",
      "993/993 [==============================] - 236s 238ms/step - loss: 0.1455 - accuracy: 0.9303 - val_loss: 0.1200 - val_accuracy: 0.9528\n",
      "Epoch 5/8\n",
      "993/993 [==============================] - 236s 238ms/step - loss: 0.1362 - accuracy: 0.9349 - val_loss: 0.1114 - val_accuracy: 0.9542\n",
      "Epoch 6/8\n",
      "993/993 [==============================] - 236s 238ms/step - loss: 0.1288 - accuracy: 0.9392 - val_loss: 0.1645 - val_accuracy: 0.9412\n",
      "Epoch 7/8\n",
      "993/993 [==============================] - 229s 231ms/step - loss: 0.1208 - accuracy: 0.9435 - val_loss: 0.1416 - val_accuracy: 0.9473\n",
      "Epoch 8/8\n",
      "993/993 [==============================] - 228s 230ms/step - loss: 0.1141 - accuracy: 0.9474 - val_loss: 0.1139 - val_accuracy: 0.9555\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_SIZE = 200\n",
    "MAX_DOCUMENT_LENGTH = 550\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 8\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(np.shape(embed_matrix)[0],\n",
    "                            EMBEDDINGS_SIZE,\n",
    "                            weights=[embed_matrix],\n",
    "                            input_length=MAX_DOCUMENT_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(LSTM(300, activation='tanh', recurrent_activation='hard_sigmoid', return_sequences=True)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(TimeDistributed(Dense(150, activation='relu', kernel_regularizer=regularizers.l2(0.01))))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(TimeDistributed(Dense(2, activation='softmax')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'],\n",
    "              sample_weight_mode=\"temporal\")\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "history = model.fit(X_train_padded, tags_train_2d,\n",
    "                    validation_data=(X_val_padded, tags_val_2d),\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    sample_weight=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 993 samples, validate on 492 samples\n",
      "Epoch 1/4\n",
      "993/993 [==============================] - 242s 244ms/step - loss: 0.1071 - accuracy: 0.9512 - val_loss: 0.1562 - val_accuracy: 0.9500\n",
      "Epoch 2/4\n",
      "993/993 [==============================] - 236s 238ms/step - loss: 0.1014 - accuracy: 0.9547 - val_loss: 0.1605 - val_accuracy: 0.9497\n",
      "Epoch 3/4\n",
      "993/993 [==============================] - 227s 229ms/step - loss: 0.0975 - accuracy: 0.9570 - val_loss: 0.1280 - val_accuracy: 0.9586\n",
      "Epoch 4/4\n",
      "993/993 [==============================] - 227s 228ms/step - loss: 0.0929 - accuracy: 0.9600 - val_loss: 0.1491 - val_accuracy: 0.9519\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded, tags_train_2d,\n",
    "                    validation_data=(X_val_padded, tags_val_2d),\n",
    "                    epochs=4,\n",
    "                    batch_size=4,\n",
    "                    sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(documents_eng, kp_eng, documents_seq, tags, model):\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    f_score = 0\n",
    "    acc = 0\n",
    "    predictions = model.predict_classes(documents_seq)\n",
    "    for idx, document_eng in enumerate(documents_eng):\n",
    "        # our document (unpadding)\n",
    "        doc_len = len(documents_eng[idx])\n",
    "        document_seq = documents_seq[idx][0:doc_len]\n",
    "        tags_predicted = predictions[idx][0:doc_len]\n",
    "        # predicted kp\n",
    "        kp_predicted = retrive_phrase_IO(tags_predicted, document_eng)\n",
    "        kp_true = kp_eng[idx]\n",
    "        tags_true = tags[idx]\n",
    "        # compute precision, recall, f_score, accuracy\n",
    "        prec += precision(kp_true, kp_predicted)\n",
    "        rec += recall(kp_true, kp_predicted)\n",
    "        f_score += f1(kp_true, kp_predicted)\n",
    "        acc += sum(np.equal(tags_true, tags_predicted))/len(tags_true)\n",
    "        #if idx == 1:\n",
    "        #    print('document_eng', document_eng)\n",
    "        #    print('document_seq', document_seq)\n",
    "        #    print(\"kp_true\",kp_true)\n",
    "        #    print(\"tags_true\" ,tags_true)\n",
    "        #    print(\"tags_predicted\", tags_predicted)\n",
    "        #    print(\"kp_predicted\", kp_predicted)\n",
    "    return prec/len(documents_eng), rec/len(documents_eng), f_score/len(documents_eng), acc/len(documents_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 0.8022458852006253\n",
      "Validation Precision 0.31943349080662076\n",
      "Validation Recall 0.6486507818586568\n",
      "Validation F-score 0.4132931757294374\n"
     ]
    }
   ],
   "source": [
    "pr, r, f, acc = validate(X_val_eng, kp_val, X_val_padded, tags_val, model)\n",
    "print('Validation Accuracy', acc)\n",
    "print('Validation Precision', pr)\n",
    "print('Validation Recall', r)\n",
    "print('Validation F-score', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 0.8050735991559653\n",
      "Validation Precision 0.32611373926613757\n",
      "Validation Recall 0.6565015695553339\n",
      "Validation F-score 0.42132533915231346\n"
     ]
    }
   ],
   "source": [
    "pr, r, f, acc = validate(X_test_eng, kp_test, X_test_padded, tags_test, model)\n",
    "print('Validation Accuracy', acc)\n",
    "print('Validation Precision', pr)\n",
    "print('Validation Recall', r)\n",
    "print('Validation F-score', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
