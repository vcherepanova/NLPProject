{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.autograd import Variable\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/valeriya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "documents = {}\n",
    "labels = {}\n",
    "p = re.compile(r\"(\\b[-']\\b)|[\\W_]\")\n",
    "\n",
    "# open main data\n",
    "for file in os.listdir(\"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training\"):\n",
    "    if file.endswith(\".abstr\"):\n",
    "        content = open((\"%s/%s\" % ('/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training', file)), \"r\").read()\n",
    "        tmp=[]\n",
    "        for s in content.split('.'):\n",
    "            tmp.append( p.sub(lambda m: (m.group(1) if m.group(1) else \" \"), s) )\n",
    "        documents[file.split('.')[0]] = tmp\n",
    "        \n",
    "# open labels        \n",
    "for file in os.listdir(\"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training\"):\n",
    "    if file.endswith(\".uncontr\"):\n",
    "        content = open((\"%s/%s\" % ('/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training', file)), \"r\").read()\n",
    "        tmp=[]\n",
    "        for s in content.split('.'):\n",
    "            tmp.append( p.sub(lambda m: (m.group(1) if m.group(1) else \" \"), s) )\n",
    "        labels[file.split('.')[0]] = tmp  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize document\n",
    "tokenized_documents = {}\n",
    "for num, ctt in documents.items():\n",
    "    tokenized_documents[num] = []\n",
    "    for sentence in ctt:\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        tokenized_documents[num].append(words)\n",
    "\n",
    "# tokenize labels\n",
    "tokenized_labels = {}\n",
    "for num, ctt in labels.items():\n",
    "    tokenized_labels[num] = []\n",
    "    for sentence in ctt:\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        tokenized_labels[num].append(words)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we preprocess labels: we label each word in each sentence with 2 if it is not a key-phrase word, \n",
    "# with 0 if it is a first word in key-phrase and 1 if it is not a first word in key-phrase.  \n",
    "\n",
    "# function finds index of element in list of lists\n",
    "def index(lab, target):\n",
    "    for i,phr in enumerate(lab):\n",
    "        for j, w in enumerate(phr):\n",
    "            if w == target:\n",
    "                return (j)\n",
    "    return (None, None)\n",
    "\n",
    "# create dictionary of labels associated to words\n",
    "class_labels = copy.deepcopy(tokenized_documents)\n",
    "for document in tokenized_documents:\n",
    "    text = tokenized_documents[document]\n",
    "    lab = tokenized_labels[document]\n",
    "    lab_flattened = [val for sublist in lab for val in sublist]\n",
    "    for i, sentence in enumerate(text): \n",
    "        for j, word in enumerate(sentence): \n",
    "            is_keyphrase = word in lab_flattened\n",
    "            if is_keyphrase:\n",
    "                if index(tokenized_labels[document], word) == 0:\n",
    "                    class_labels[document][i][j] = 0\n",
    "                else:\n",
    "                    class_labels[document][i][j] = 1\n",
    "            else:\n",
    "                class_labels[document][i][j] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOVE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here we download pretrained glove embeddings\n",
    "import numpy as np\n",
    "embeddings = dict()\n",
    "embed_size = 100\n",
    "f = open('glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings[word] = coefs\n",
    "f.close()\n",
    "\n",
    "glove_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our data\n",
    "X = [sent for doc in copy.deepcopy(tokenized_documents).values() for sent in doc if sent!= []]\n",
    "y = [sent for doc in copy.deepcopy(class_labels).values() for sent in doc if sent!=[]]\n",
    "\n",
    "# our vocab: all the words in all abstracts\n",
    "target_vocab = list(set([item for sublist in X for item in sublist]))\n",
    "# dictionary with all words and their indices\n",
    "vocab_ind_dict = dict(zip(target_vocab, range(0, len(target_vocab)))) \n",
    "\n",
    "\n",
    "# create matrix of glove vectors + random vectors for the words which are in vocab but not in glove\n",
    "\n",
    "matrix_len = len(target_vocab)\n",
    "weights_glove = np.zeros((matrix_len, glove_size))\n",
    "\n",
    "for i, word in enumerate(target_vocab):\n",
    "    try: \n",
    "        weights_glove[i] = embeddings[word]\n",
    "    except KeyError:\n",
    "        weights_glove[i] = np.random.normal(scale=0.6, size=(glove_size, ))\n",
    "\n",
    "# replace words in our data with their indices\n",
    "for i, sent in enumerate(X):\n",
    "    for j, word in enumerate(sent):\n",
    "        X[i][j] = vocab_ind_dict[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, nb_layers = 1, nb_lstm_units=150, nb_lin_units=150, embedding_dim=100, batch_size=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.vocab = target_vocab\n",
    "        self.tags = {'first': 0, 'inside': 1, 'no': 2}\n",
    "        self.nb_lstm_layers = nb_layers\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.nb_lin_units = nb_lin_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_tags = len(self.tags)\n",
    "        nb_vocab_words = len(self.vocab)\n",
    "\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=nb_vocab_words,\n",
    "            embedding_dim=self.embedding_dim)\n",
    "        self.word_embedding.load_state_dict({'weight': torch.Tensor(weights_glove)})\n",
    "        self.word_embedding.weight.requires_grad = False\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            num_layers=self.nb_lstm_layers,\n",
    "            batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.drop = torch.nn.Dropout(p=0.25)\n",
    "\n",
    "        # linear layers\n",
    "        self.lin1 = nn.Linear(2*self.nb_lstm_units, self.nb_lin_units)\n",
    "        self.lin2 = nn.Linear(self.nb_lstm_units, self.nb_tags)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden_a = torch.randn(2, self.batch_size, self.nb_lstm_units).to(device)\n",
    "        hidden_b = torch.randn(2, self.batch_size, self.nb_lstm_units).to(device)\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "\n",
    "        return (hidden_a, hidden_b)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "        X = self.word_embedding(X)\n",
    "        batch_size, seq_len, _ = X.size()\n",
    "\n",
    "        X, self.hidden = self.lstm(X, self.hidden)\n",
    "        X = self.drop(X)\n",
    "        X = X.contiguous()\n",
    "        X = X.view(-1, X.shape[2])\n",
    "\n",
    "        X = F.relu(self.lin1(X))\n",
    "        X = F.relu(self.lin2(X))\n",
    "\n",
    "        X = X.view(batch_size, self.nb_tags, -1)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LSTM().to(device)\n",
    "criterion= torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "tb = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function should be here \n",
    "# Something like \n",
    "\n",
    "def train(epoch):\n",
    "    train_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, sent in enumerate(X):\n",
    "        inputs = torch.LongTensor(sent).view([1, len(sent)]).to(device)\n",
    "        labels = torch.LongTensor(y[i]).view([1, len(y[i])]).to(device)     \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # compute predictions\n",
    "        outputs = (net(inputs))\n",
    "        # compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # statistics to display\n",
    "        train_loss += loss.item()\n",
    "        _,predicted = outputs.max(1)\n",
    "        total += labels[0].size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    print('Train accuracy:', acc)\n",
    "    print('Loss:', train_loss)\n",
    "    tb.add_scalar('Loss', train_loss, epoch)\n",
    "    tb.add_scalar('Accuracy', acc, epoch)\n",
    "    return(train_loss)\n",
    "\n",
    "\n",
    "def main(num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('\\nEpoch: %d' % epoch)\n",
    "        train_loss = train(epoch)\n",
    "    tb.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Train accuracy: 73.72410269115234\n",
      "Loss: 6296.4189068220485\n",
      "\n",
      "Epoch: 1\n",
      "Train accuracy: 74.9326437707101\n",
      "Loss: 6416.993394136429\n",
      "\n",
      "Epoch: 2\n"
     ]
    }
   ],
   "source": [
    "main(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
