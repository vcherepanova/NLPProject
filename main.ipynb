{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.autograd import Variable\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/valeriya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Here we download the data and split it into words, still need to get rid of punctuation\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "documents = {}\n",
    "labels = {}\n",
    "\n",
    "# open main data\n",
    "for file in os.listdir(\"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training\"):\n",
    "    if file.endswith(\".abstr\"):\n",
    "        content = open((\"%s/%s\" % ('/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training', file)), \"r\").read()\n",
    "        documents[file.split('.')[0]] = content.split('. ')\n",
    "        \n",
    "# open labels        \n",
    "for file in os.listdir(\"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training\"):\n",
    "    if file.endswith(\".uncontr\"):\n",
    "        content = open((\"%s/%s\" % ('/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training', file)), \"r\").read()\n",
    "        labels[file.split('.')[0]] = content.split(\"; \")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize document\n",
    "tokenized_documents = {}\n",
    "for num, ctt in documents.items():\n",
    "    tokenized_documents[num] = []\n",
    "    for sentence in ctt:\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        tokenized_documents[num].append(words)\n",
    "\n",
    "# tokenize labels\n",
    "tokenized_labels = {}\n",
    "for num, ctt in labels.items():\n",
    "    tokenized_labels[num] = []\n",
    "    for sentence in ctt:\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        tokenized_labels[num].append(words)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we preprocess labels: we label each word in each sentence with \"no\" if it is not a key-phrase word, \n",
    "# with \"first\" if it is a first word in key-phrase and \"inside\" if it is not a first word in key-phrase.  \n",
    "\n",
    "# function finds index of element in list of lists\n",
    "def index(lab, target):\n",
    "    for i,phr in enumerate(lab):\n",
    "        for j, w in enumerate(phr):\n",
    "            if w == target:\n",
    "                return (j)\n",
    "    return (None, None)\n",
    "\n",
    "# create dictionary of labels associated to words\n",
    "class_labels = copy.deepcopy(tokenized_documents)\n",
    "for document in tokenized_documents:\n",
    "    text = tokenized_documents[document]\n",
    "    lab = tokenized_labels[document]\n",
    "    lab_flattened = [val for sublist in lab for val in sublist]\n",
    "    for i, sentence in enumerate(text): \n",
    "        for j, word in enumerate(sentence): \n",
    "            is_keyphrase = word in lab_flattened\n",
    "            if is_keyphrase:\n",
    "                if index(tokenized_labels[document], word) == 0:\n",
    "                    class_labels[document][i][j] = \"first\"\n",
    "                else:\n",
    "                    class_labels[document][i][j] = 'inside'\n",
    "            else:\n",
    "                class_labels[document][i][j] = 'no'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOVE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here we download pretrained glove embeddings\n",
    "import numpy as np\n",
    "embeddings = dict()\n",
    "embed_size = 100\n",
    "f = open('glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we pad our data: take maximum length of sentence (l) and if length of current sentence (n) is less => \n",
    "# we add word \"PAD\" l-n times to make all sentences having the same length\n",
    "\n",
    "# our data\n",
    "X = [sent for doc in copy.deepcopy(tokenized_documents).values() for sent in doc]\n",
    "y = [sent for doc in copy.deepcopy(class_labels).values() for sent in doc]\n",
    "\n",
    "# padding the data\n",
    "def Padding(data):\n",
    "    len_max = len(max(X, key=lambda coll: len(coll)))\n",
    "    for i, sentence in enumerate(data):\n",
    "        len_sent = len(sentence)\n",
    "        len_pad = len_max - len_sent\n",
    "        sentence.extend(['PAD' for i in range(len_pad)])\n",
    "    return(data)\n",
    "\n",
    "X_padded = Padding(X)\n",
    "y_padded = Padding(y)\n",
    "X_lengths = len(X_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our vocab: all the words in all abstracts\n",
    "target_vocab = set([item for sublist in X_padded for item in sublist])\n",
    "\n",
    "# create matrix of glove vectors + random vectors for the words which are in vocab but not in glove\n",
    "# this matrix is used in the embedding layer\n",
    "\n",
    "matrix_len = len(target_vocab)\n",
    "weights_glove = np.zeros((matrix_len, 100))\n",
    "\n",
    "for i, word in enumerate(target_vocab):\n",
    "    try: \n",
    "        weights_glove[i] = embeddings[word]\n",
    "    except KeyError:\n",
    "        weights_glove[i] = np.random.normal(scale=0.6, size=(100, ))\n",
    "\n",
    "# find which vector corresponds to \"PAD\":\n",
    "pad_idx = list(target_vocab).index('PAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we replace words with their indices\n",
    "for i, sent in enumerate(X_padded):\n",
    "    for j, word in enumerate(sent):\n",
    "        X_padded[i][j] = list(target_vocab).index(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, nb_layers = 1, nb_lstm_units=150, nb_lin_units=150, embedding_dim=100, batch_size=1,\n",
    "                 pad_idx = pad_idx):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.vocab = target_vocab\n",
    "        self.tags = {'PAD': 0, 'first': 1, 'inside': 2, 'no': 3}\n",
    "        \n",
    "        self.nb_lstm_layers = nb_layers\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.nb_lin_units = nb_lin_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # don't count the padding tag for the classifier output\n",
    "        self.nb_tags = len(self.tags) - 1\n",
    "\n",
    "\n",
    "        nb_vocab_words = len(self.vocab)\n",
    "\n",
    "        # whenever the embedding sees the padding index it'll make the whole vector zeros\n",
    "        padding_idx = pad_idx\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=nb_vocab_words,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        \n",
    "        self.word_embedding.load_state_dict({'weight': torch.Tensor(weights_glove)})\n",
    "        self.word_embedding.weight.requires_grad = False\n",
    "\n",
    "        # design LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            num_layers=self.nb_lstm_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # output layer which projects back to tag space\n",
    "        self.lin1 = nn.Linear(self.nb_lstm_units, self.nb_lin_units)\n",
    "        self.lin2 = nn.Linear(self.nb_lstm_units, self.nb_tags)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden_a = torch.randn(self.nb_lstm_layers, self.batch_size, self.nb_lstm_units).to(device)\n",
    "        hidden_b = torch.randn(self.nb_lstm_layers, self.batch_size, self.nb_lstm_units).to(device)\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "\n",
    "        return (hidden_a, hidden_b)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # reset the LSTM hidden state. Must be done before you run a new batch. Otherwise the LSTM will treat\n",
    "        # a new batch as a continuation of a sequence\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        X_len = torch.LongTensor([43]).to(device) # need to compute  = list of sentence lengths before padding, for ex [8,4,6]\n",
    "\n",
    "        # 1. embed the input\n",
    "        X = self.word_embedding(X)\n",
    "        batch_size, seq_len, _ = X.size()\n",
    "\n",
    "        # 2. Run through RNN\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        X = torch.nn.utils.rnn.pack_padded_sequence(X, X_len, batch_first=True)\n",
    "\n",
    "        # now run through LSTM\n",
    "        X, self.hidden = self.lstm(X, self.hidden)\n",
    "\n",
    "        # undo the packing operation\n",
    "        X, _ = torch.nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "\n",
    "        # 3. Project to tag space\n",
    "        # this one is a bit tricky as well. First we need to reshape the data so it goes into the linear layer\n",
    "        X = X.contiguous()\n",
    "        X = X.view(-1, X.shape[2])\n",
    "\n",
    "        # run through actual linear layer\n",
    "        X = F.relu(self.lin1(X))\n",
    "        X = F.relu(self.lin2(X))\n",
    "\n",
    "        # I like to reshape for mental sanity so we're back to (batch_size, seq_len, nb_tags)\n",
    "        X = X.view(batch_size, X_len, self.nb_tags)\n",
    "\n",
    "        Y_hat = X\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valeriya/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "net = LSTM().to(device)\n",
    "# here we define our loss function and optimizer\n",
    "criterion= torch.nn.CrossEntropyLoss(size_average=True, ignore_index=0)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.1418, 0.0797],\n",
       "         [0.0007, 0.0811, 0.0861],\n",
       "         [0.0214, 0.0691, 0.0879],\n",
       "         [0.0038, 0.0722, 0.0521],\n",
       "         [0.0000, 0.0809, 0.0520],\n",
       "         [0.0034, 0.0624, 0.0463],\n",
       "         [0.0017, 0.0692, 0.0513],\n",
       "         [0.0071, 0.0801, 0.0403],\n",
       "         [0.0155, 0.0792, 0.0221],\n",
       "         [0.0000, 0.0840, 0.0299],\n",
       "         [0.0000, 0.0931, 0.0449],\n",
       "         [0.0044, 0.0652, 0.0300],\n",
       "         [0.0182, 0.0707, 0.0438],\n",
       "         [0.0037, 0.0797, 0.0265],\n",
       "         [0.0014, 0.0915, 0.0243],\n",
       "         [0.0007, 0.0741, 0.0374],\n",
       "         [0.0000, 0.0817, 0.0392],\n",
       "         [0.0000, 0.0825, 0.0313],\n",
       "         [0.0064, 0.0775, 0.0150],\n",
       "         [0.0216, 0.0805, 0.0349],\n",
       "         [0.0073, 0.0620, 0.0342],\n",
       "         [0.0008, 0.0702, 0.0446],\n",
       "         [0.0000, 0.0830, 0.0560],\n",
       "         [0.0049, 0.0587, 0.0364],\n",
       "         [0.0192, 0.0671, 0.0477],\n",
       "         [0.0041, 0.0613, 0.0382],\n",
       "         [0.0000, 0.0614, 0.0310],\n",
       "         [0.0000, 0.0618, 0.0272],\n",
       "         [0.0000, 0.0785, 0.0258],\n",
       "         [0.0254, 0.0794, 0.0089],\n",
       "         [0.0000, 0.0894, 0.0263],\n",
       "         [0.0064, 0.0853, 0.0322],\n",
       "         [0.0012, 0.0873, 0.0425],\n",
       "         [0.0000, 0.0877, 0.0316],\n",
       "         [0.0261, 0.0894, 0.0294],\n",
       "         [0.0201, 0.0831, 0.0500],\n",
       "         [0.0251, 0.0807, 0.0359],\n",
       "         [0.0089, 0.0788, 0.0236],\n",
       "         [0.0120, 0.0692, 0.0398],\n",
       "         [0.0043, 0.0731, 0.0381],\n",
       "         [0.0000, 0.0768, 0.0292],\n",
       "         [0.0187, 0.0528, 0.0203],\n",
       "         [0.0262, 0.0504, 0.0552]]], device='cuda:0', grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run network on one example \n",
    "data_1 = torch.LongTensor(X_padded[0]).view([1, 125]).to(device)\n",
    "net(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training function should be here \n",
    "# Something like \n",
    "\n",
    "def train(epoch):\n",
    "    train_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, data in # DATA\n",
    "        # get sequence of inputs and labels from the data\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)     \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # compute predictions\n",
    "        outputs = net(inputs)\n",
    "        # compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # statistics to display\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    print('Train accuracy:', acc)\n",
    "    tb.add_scalar('Loss', train_loss, epoch)\n",
    "    tb.add_scalar('Accuracy', acc, epoch)\n",
    "    print(net.conv1.bias.grad.norm())\n",
    "    return(train_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('\\nEpoch: %d' % epoch)\n",
    "        train_loss = train(epoch)\n",
    "        print(train_loss)\n",
    "        test_accuracy = test()\n",
    "    tb.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
