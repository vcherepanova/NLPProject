{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.autograd import Variable\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/valeriya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "documents = {}\n",
    "labels = {}\n",
    "p = re.compile(r\"(\\b[-']\\b)|[\\W_]\")\n",
    "\n",
    "#directories\n",
    "dir_valeria = \"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training\"\n",
    "dir_anna = \"/Users/annasotnikova/Downloads/Hulth2003/Training\"\n",
    "\n",
    "\n",
    "# finction opening the data, splitting it using \".\"\n",
    "def open_data(directory):\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".abstr\"):\n",
    "            content = open((\"%s/%s\" % (directory, file)), \"r\").read()\n",
    "            tmp=[]\n",
    "            for s in content.split('.'):\n",
    "                tmp.append( p.sub(lambda m: (m.group(1) if m.group(1) else \" \"), s) )\n",
    "            documents[file.split('.')[0]] = tmp\n",
    "\n",
    "    # open labels        \n",
    "    # open training labels        \n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".uncontr\"):\n",
    "            content = open((\"%s/%s\" % (directory, file)), \"r\").read()\n",
    "            labels[file.split('.')[0]] = content.split(\"; \")\n",
    "            \n",
    "    return documents, labels\n",
    "\n",
    "# tokenization function\n",
    "def tokenize(doc):\n",
    "    tokenized_doc = {}\n",
    "    for num, ctt in doc.items():\n",
    "        tokenized_doc[num] = []\n",
    "        for sentence in ctt:\n",
    "            words = nltk.word_tokenize(sentence.lower())\n",
    "            tokenized_doc[num].append(words)\n",
    "    return tokenized_doc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, labels = open_data(dir_valeria)\n",
    "tokenized_documents = tokenize(documents)\n",
    "tokenized_labels = tokenize(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we preprocess labels: we label each word in each sentence with 2 if it is not a key-phrase word, \n",
    "# with 0 if it is a first word in key-phrase and 1 if it is not a first word in key-phrase.  \n",
    "\n",
    "# function finds index of element in list of lists\n",
    "def index(lab, target):\n",
    "    for i,phr in enumerate(lab):\n",
    "        for j, w in enumerate(phr):\n",
    "            if w == target:\n",
    "                return (j)\n",
    "    return (None, None)\n",
    "\n",
    "\n",
    "def tag_labels(tokenized_documents, tokenized_labels):\n",
    "    # create dictionary of labels associated to words\n",
    "    class_labels = copy.deepcopy(tokenized_documents)\n",
    "    for document in tokenized_documents:\n",
    "        # take one document\n",
    "        text = tokenized_documents[document]\n",
    "        lab = tokenized_labels[document]\n",
    "        lab_flattened = [val for sublist in lab for val in sublist]\n",
    "        for i, sentence in enumerate(text): \n",
    "            for j, word in enumerate(sentence): \n",
    "                is_keyphrase = word in lab_flattened\n",
    "                if is_keyphrase:\n",
    "                    if index(tokenized_labels[document], word) == 0:\n",
    "                        class_labels[document][i][j] = 1\n",
    "                    else:\n",
    "                        class_labels[document][i][j] = 2\n",
    "                else:\n",
    "                    class_labels[document][i][j] = 0\n",
    "    return class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = tag_labels(tokenized_documents, tokenized_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero 97169\n",
      "one 11942\n",
      "two 20053\n"
     ]
    }
   ],
   "source": [
    "lab_flattened =  [val for sublist in class_labels.values() for val in sublist] \n",
    "lab_flattened  = [val for sublist in lab_flattened for val in sublist] \n",
    "print('zero', lab_flattened.count(0))\n",
    "print('one', lab_flattened.count(1))\n",
    "print('two', lab_flattened.count(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOVE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here we download pretrained glove embeddings\n",
    "import numpy as np\n",
    "embeddings = dict()\n",
    "embed_size = 100\n",
    "f = open('glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings[word] = coefs\n",
    "f.close()\n",
    "\n",
    "glove_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our data\n",
    "X = [sent for doc in copy.deepcopy(tokenized_documents).values() for sent in doc if sent!= []]\n",
    "y = [sent for doc in copy.deepcopy(class_labels).values() for sent in doc if sent!=[]]\n",
    "\n",
    "# our vocab: all the words in all abstracts\n",
    "target_vocab = list(set([item for sublist in X for item in sublist]))\n",
    "# dictionary with all words and their indices\n",
    "vocab_ind_dict = dict(zip(target_vocab, range(0, len(target_vocab)))) \n",
    "\n",
    "\n",
    "# create matrix of glove vectors + random vectors for the words which are in vocab but not in glove\n",
    "\n",
    "matrix_len = len(target_vocab)\n",
    "weights_glove = np.zeros((matrix_len, glove_size))\n",
    "\n",
    "for i, word in enumerate(target_vocab):\n",
    "    try: \n",
    "        weights_glove[i] = embeddings[word]\n",
    "    except KeyError:\n",
    "        weights_glove[i] = np.random.normal(scale=0.6, size=(glove_size, ))\n",
    "\n",
    "# replace words in our data with their indices\n",
    "for i, sent in enumerate(X):\n",
    "    for j, word in enumerate(sent):\n",
    "        X[i][j] = vocab_ind_dict[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, nb_layers = 1, nb_lstm_units=150, nb_lin_units=150, embedding_dim=100, batch_size=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.vocab = target_vocab\n",
    "        self.tags = {'first': 1, 'inside': 2, 'no': 0}\n",
    "        self.nb_lstm_layers = nb_layers\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.nb_lin_units = nb_lin_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_tags = len(self.tags)\n",
    "        nb_vocab_words = len(self.vocab)\n",
    "\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=nb_vocab_words,\n",
    "            embedding_dim=self.embedding_dim)\n",
    "        self.word_embedding.load_state_dict({'weight': torch.Tensor(weights_glove)})\n",
    "        self.word_embedding.weight.requires_grad = True\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            num_layers=self.nb_lstm_layers,\n",
    "            batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.drop = torch.nn.Dropout(p=0.25)\n",
    "\n",
    "        # linear layers\n",
    "        self.lin1 = nn.Linear(2*self.nb_lstm_units, self.nb_lin_units)\n",
    "        self.lin2 = nn.Linear(self.nb_lstm_units, self.nb_tags)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden_a = torch.randn(2, self.batch_size, self.nb_lstm_units).to(device)\n",
    "        hidden_b = torch.randn(2, self.batch_size, self.nb_lstm_units).to(device)\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "\n",
    "        return (hidden_a, hidden_b)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "        X = self.word_embedding(X)\n",
    "        batch_size, seq_len, _ = X.size()\n",
    "\n",
    "        X, self.hidden = self.lstm(X, self.hidden)\n",
    "        X = self.drop(X)\n",
    "        X = X.contiguous()\n",
    "        X = X.view(-1, X.shape[2])\n",
    "\n",
    "        X = F.relu(self.lin1(X))\n",
    "        X = F.relu(self.lin2(X))\n",
    "\n",
    "        X = X.view(batch_size, self.nb_tags, -1)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = LSTM().to(device)\n",
    "weight = torch.tensor([1/50, 1, 1/2]).to(device)\n",
    "criterion= torch.nn.CrossEntropyLoss(weight=weight)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "tb = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training function should be here \n",
    "# Something like \n",
    "\n",
    "def train(epoch):\n",
    "    train_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, sent in enumerate(X):\n",
    "        inputs = torch.LongTensor(sent).view([1, len(sent)]).to(device)\n",
    "        labels = torch.LongTensor(y[i]).view([1, len(y[i])]).to(device)     \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # compute predictions\n",
    "        outputs = (net(inputs))\n",
    "        # compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # statistics to display\n",
    "        train_loss += loss.item()\n",
    "        _,predicted = outputs.max(1)\n",
    "        total += labels[0].size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        if i ==0:\n",
    "            print(predicted)\n",
    "            print(outputs)\n",
    "            print(labels)\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    print('Train accuracy:', acc)\n",
    "    print('Loss:', train_loss)\n",
    "    tb.add_scalar('Loss', train_loss, epoch)\n",
    "    tb.add_scalar('Accuracy', acc, epoch)\n",
    "    return(train_loss)\n",
    "\n",
    "\n",
    "def main(num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('\\nEpoch: %d' % epoch)\n",
    "        train_loss = train(epoch)\n",
    "    tb.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "tensor([[0, 0, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 0, 2, 0, 1, 1, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 0, 2, 0, 1, 2, 0, 1, 1, 0, 1]], device='cuda:0')\n",
      "tensor([[[0.0636, 0.0403, 0.0000, 0.0606, 0.0106, 0.0000, 0.0412, 0.0185,\n",
      "          0.0000, 0.0479, 0.0000, 0.0000, 0.0478, 0.0238, 0.0000, 0.0703,\n",
      "          0.0105, 0.0000, 0.0470, 0.0000, 0.0000, 0.0415, 0.0017, 0.0000,\n",
      "          0.0639, 0.0033, 0.0000, 0.0266, 0.0114, 0.0000, 0.0358, 0.0000,\n",
      "          0.0000, 0.0321, 0.0140, 0.0000, 0.0350, 0.0153],\n",
      "         [0.0000, 0.0000, 0.0051, 0.0000, 0.0304, 0.0294, 0.0000, 0.0470,\n",
      "          0.0308, 0.0000, 0.0344, 0.0182, 0.0000, 0.0169, 0.0047, 0.0000,\n",
      "          0.0472, 0.0430, 0.0000, 0.0416, 0.0190, 0.0000, 0.0492, 0.0034,\n",
      "          0.0000, 0.0303, 0.0145, 0.0000, 0.0098, 0.0290, 0.0000, 0.0078,\n",
      "          0.0000, 0.0000, 0.0214, 0.0156, 0.0000, 0.0371],\n",
      "         [0.0204, 0.0000, 0.0363, 0.0000, 0.0000, 0.0563, 0.0000, 0.0000,\n",
      "          0.0348, 0.0271, 0.0000, 0.0361, 0.0000, 0.0000, 0.0394, 0.0000,\n",
      "          0.0000, 0.0320, 0.0000, 0.0000, 0.0304, 0.0000, 0.0000, 0.0445,\n",
      "          0.0000, 0.0000, 0.0509, 0.0000, 0.0000, 0.0324, 0.0000, 0.0000,\n",
      "          0.0274, 0.0102, 0.0000, 0.0098, 0.0000, 0.0000]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "tensor([[1, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0,\n",
      "         0, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 2]], device='cuda:0')\n",
      "Train accuracy: 44.35291567309777\n",
      "Loss: 6393.847602069378\n",
      "\n",
      "Epoch: 1\n",
      "tensor([[0, 2, 0, 0, 2, 0, 0, 2, 0, 1, 2, 0, 1, 1, 0, 0, 1, 0, 1, 2, 0, 1, 2, 0,\n",
      "         1, 1, 0, 1, 2, 2, 1, 1, 2, 0, 1, 2, 1, 1]], device='cuda:0')\n",
      "tensor([[[0.1920, 0.0000, 0.0542, 0.2293, 0.0000, 0.0198, 0.1396, 0.0000,\n",
      "          0.0608, 0.0282, 0.0000, 0.0812, 0.0498, 0.0000, 0.0720, 0.0672,\n",
      "          0.0000, 0.0763, 0.0411, 0.0000, 0.0867, 0.0000, 0.0000, 0.0925,\n",
      "          0.0450, 0.0000, 0.0956, 0.0280, 0.0000, 0.0639, 0.0156, 0.0000,\n",
      "          0.0967, 0.0541, 0.0000, 0.0743, 0.0298, 0.0000],\n",
      "         [0.0847, 0.0099, 0.0000, 0.1070, 0.0000, 0.0000, 0.1045, 0.0202,\n",
      "          0.0000, 0.1050, 0.0000, 0.0000, 0.0586, 0.1396, 0.0000, 0.0562,\n",
      "          0.1546, 0.0000, 0.0822, 0.0423, 0.0000, 0.0787, 0.0220, 0.0000,\n",
      "          0.0971, 0.0964, 0.0000, 0.0383, 0.0335, 0.0000, 0.0730, 0.0997,\n",
      "          0.0000, 0.0304, 0.0229, 0.0000, 0.0737, 0.0898],\n",
      "         [0.0000, 0.1053, 0.0203, 0.0000, 0.0867, 0.0198, 0.0000, 0.0606,\n",
      "          0.0000, 0.0000, 0.0661, 0.0256, 0.0000, 0.0513, 0.0000, 0.0000,\n",
      "          0.0706, 0.0000, 0.0000, 0.0954, 0.0000, 0.0000, 0.0735, 0.0323,\n",
      "          0.0000, 0.0578, 0.0013, 0.0000, 0.1047, 0.0821, 0.0000, 0.0853,\n",
      "          0.2176, 0.0000, 0.0181, 0.3643, 0.0000, 0.0000]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "tensor([[1, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0,\n",
      "         0, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 2]], device='cuda:0')\n",
      "Train accuracy: 36.99250565172959\n",
      "Loss: 6344.115755975246\n",
      "\n",
      "Epoch: 2\n",
      "tensor([[0, 2, 0, 0, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 1, 0, 1, 2, 0, 1, 2, 0,\n",
      "         1, 2, 0, 1, 2, 0, 1, 2, 2, 1, 2, 2, 1, 2]], device='cuda:0')\n",
      "tensor([[[1.7593e-01, 0.0000e+00, 0.0000e+00, 1.1810e-01, 0.0000e+00,\n",
      "          4.6743e-02, 4.9075e-02, 0.0000e+00, 4.0168e-04, 0.0000e+00,\n",
      "          0.0000e+00, 2.3875e-02, 0.0000e+00, 0.0000e+00, 1.0526e-01,\n",
      "          0.0000e+00, 0.0000e+00, 9.4733e-02, 0.0000e+00, 0.0000e+00,\n",
      "          1.4977e-01, 0.0000e+00, 0.0000e+00, 7.3743e-02, 0.0000e+00,\n",
      "          0.0000e+00, 1.4623e-01, 0.0000e+00, 0.0000e+00, 1.7480e-01,\n",
      "          0.0000e+00, 0.0000e+00, 2.0031e-01, 0.0000e+00, 0.0000e+00,\n",
      "          1.6376e-01, 0.0000e+00, 0.0000e+00],\n",
      "         [8.3323e-02, 0.0000e+00, 0.0000e+00, 8.0228e-02, 0.0000e+00,\n",
      "          0.0000e+00, 2.0365e-01, 2.7187e-02, 0.0000e+00, 1.5382e-01,\n",
      "          0.0000e+00, 0.0000e+00, 1.1566e-01, 4.0353e-02, 0.0000e+00,\n",
      "          1.9432e-01, 1.6202e-01, 0.0000e+00, 1.7102e-01, 0.0000e+00,\n",
      "          0.0000e+00, 2.0122e-01, 5.2086e-03, 0.0000e+00, 2.4512e-01,\n",
      "          6.1252e-02, 0.0000e+00, 1.6653e-01, 0.0000e+00, 0.0000e+00,\n",
      "          1.3360e-01, 4.3460e-02, 0.0000e+00, 1.9804e-01, 0.0000e+00,\n",
      "          0.0000e+00, 1.2047e-01, 1.0027e-01],\n",
      "         [0.0000e+00, 1.4631e-01, 0.0000e+00, 0.0000e+00, 3.1962e-02,\n",
      "          0.0000e+00, 0.0000e+00, 8.5382e-02, 0.0000e+00, 0.0000e+00,\n",
      "          1.1548e-01, 1.1069e-02, 0.0000e+00, 1.6787e-01, 0.0000e+00,\n",
      "          0.0000e+00, 8.3984e-02, 0.0000e+00, 0.0000e+00, 1.0586e-01,\n",
      "          0.0000e+00, 0.0000e+00, 1.8830e-01, 0.0000e+00, 0.0000e+00,\n",
      "          1.5399e-01, 0.0000e+00, 0.0000e+00, 1.9769e-01, 8.2363e-02,\n",
      "          0.0000e+00, 3.9612e-01, 3.3472e-01, 0.0000e+00, 3.4525e-01,\n",
      "          4.9302e-01, 0.0000e+00, 2.2228e-01]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "tensor([[1, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0,\n",
      "         0, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 2]], device='cuda:0')\n",
      "Train accuracy: 30.79728097612338\n",
      "Loss: 6253.212566375732\n",
      "\n",
      "Epoch: 3\n",
      "tensor([[1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 0, 1, 1, 2, 1, 2, 2,\n",
      "         1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2]], device='cuda:0')\n",
      "tensor([[[0.3411, 0.0000, 0.0276, 0.1267, 0.0000, 0.0000, 0.1519, 0.0000,\n",
      "          0.0000, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1356,\n",
      "          0.0000, 0.1926, 0.2725, 0.0000, 0.1428, 0.2026, 0.0000, 0.0337,\n",
      "          0.2085, 0.0000, 0.0392, 0.1265, 0.0000, 0.1548, 0.1953, 0.0000,\n",
      "          0.5082, 0.2355, 0.0000, 0.4291, 0.3222, 0.0000],\n",
      "         [0.3555, 0.5223, 0.0000, 0.3630, 0.3302, 0.0000, 0.4803, 0.4654,\n",
      "          0.0000, 0.6211, 0.2475, 0.0000, 0.2979, 0.3312, 0.0000, 0.3715,\n",
      "          0.2975, 0.0000, 0.3706, 0.2153, 0.0000, 0.2071, 0.1038, 0.0000,\n",
      "          0.4795, 0.3230, 0.0000, 0.4959, 0.3084, 0.0000, 0.4465, 0.5020,\n",
      "          0.0000, 0.4636, 0.3925, 0.0000, 0.4085, 0.4835],\n",
      "         [0.0000, 0.4247, 0.3811, 0.0000, 0.0687, 0.3010, 0.0000, 0.1620,\n",
      "          0.1352, 0.0000, 0.0679, 0.1961, 0.0000, 0.1461, 0.0833, 0.0000,\n",
      "          0.0405, 0.1761, 0.0000, 0.0581, 0.1489, 0.0000, 0.1673, 0.2256,\n",
      "          0.0000, 0.2665, 0.2836, 0.0000, 0.2153, 0.4894, 0.0000, 0.9896,\n",
      "          0.5539, 0.0000, 1.0455, 0.7533, 0.0000, 0.7395]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "tensor([[1, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0,\n",
      "         0, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 2]], device='cuda:0')\n",
      "Train accuracy: 27.488309436065776\n",
      "Loss: 6162.550189148635\n",
      "\n",
      "Epoch: 4\n",
      "tensor([[1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2,\n",
      "         1, 2, 2, 1, 2, 2, 1, 2, 0, 1, 2, 0, 0, 2]], device='cuda:0')\n",
      "tensor([[[0.0381, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0203, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0422,\n",
      "          0.0000, 0.0300, 0.3563, 0.0000, 0.1083, 0.3069, 0.0000, 0.1021,\n",
      "          0.2673, 0.0000, 0.0257, 0.2029, 0.0000, 0.0628, 0.2601, 0.0000,\n",
      "          0.7392, 0.4083, 0.0000, 0.5757, 0.6118, 0.0000],\n",
      "         [0.5552, 0.6239, 0.0000, 0.5564, 0.4050, 0.0000, 0.4040, 0.4469,\n",
      "          0.0000, 0.5048, 0.4638, 0.0000, 0.2114, 0.4697, 0.0000, 0.4342,\n",
      "          0.4167, 0.0000, 0.5257, 0.3641, 0.0000, 0.3829, 0.2381, 0.0000,\n",
      "          0.7086, 0.4636, 0.0000, 0.5525, 0.5351, 0.0000, 0.7268, 0.6693,\n",
      "          0.0000, 0.8316, 0.5011, 0.0000, 0.4570, 0.6920],\n",
      "         [0.0000, 0.6263, 0.8488, 0.0000, 0.5794, 0.4183, 0.0000, 0.3246,\n",
      "          0.5465, 0.0000, 0.5221, 0.5720, 0.0000, 0.5824, 0.4111, 0.0000,\n",
      "          0.2180, 0.3778, 0.0000, 0.3937, 0.3669, 0.0000, 0.2987, 0.3187,\n",
      "          0.0000, 0.4711, 0.4257, 0.0000, 0.6051, 0.4858, 0.0000, 0.9987,\n",
      "          0.4525, 0.0000, 0.9897, 0.5726, 0.0000, 1.0278]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "tensor([[1, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0,\n",
      "         0, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 2]], device='cuda:0')\n",
      "Train accuracy: 27.671022885633768\n",
      "Loss: 6113.141547750682\n",
      "\n",
      "Epoch: 5\n",
      "tensor([[1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 0, 1, 1, 2, 1, 2, 2,\n",
      "         1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 0, 0, 2]], device='cuda:0')\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0190, 0.0000, 0.0000, 0.0313, 0.0000, 0.0000, 0.2449,\n",
      "          0.0000, 0.4715, 0.3872, 0.0000, 0.2604, 0.3812, 0.0000, 0.0062,\n",
      "          0.3601, 0.0000, 0.1125, 0.4726, 0.0000, 0.1752, 0.2184, 0.0000,\n",
      "          0.3514, 0.4866, 0.0000, 0.6524, 0.6899, 0.0000],\n",
      "         [0.8250, 0.8210, 0.0000, 0.4366, 0.5904, 0.0000, 0.4932, 0.6127,\n",
      "          0.0000, 0.5056, 0.5309, 0.0000, 0.4768, 0.4778, 0.0000, 0.2804,\n",
      "          0.3565, 0.0000, 0.4922, 0.6140, 0.0000, 0.4193, 0.3333, 0.0000,\n",
      "          0.6295, 0.3785, 0.0000, 0.6622, 0.5150, 0.0000, 0.7049, 0.6826,\n",
      "          0.0000, 0.7968, 0.8624, 0.0000, 0.6331, 0.7343],\n",
      "         [0.0000, 0.5909, 0.6958, 0.0000, 0.4912, 0.4768, 0.0000, 0.2487,\n",
      "          0.4560, 0.0000, 0.2455, 0.3864, 0.0000, 0.3584, 0.4059, 0.0000,\n",
      "          0.4449, 0.4476, 0.0000, 0.4942, 0.4248, 0.0000, 0.3716, 0.4100,\n",
      "          0.0000, 0.6112, 0.4908, 0.0000, 0.3508, 0.2042, 0.0000, 0.8376,\n",
      "          0.5006, 0.0000, 1.0219, 0.5589, 0.0000, 1.2817]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "tensor([[1, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0,\n",
      "         0, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 2]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 27.671797095165836\n",
      "Loss: 6073.353940103203\n",
      "\n",
      "Epoch: 6\n",
      "tensor([[1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2,\n",
      "         1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 0, 0, 2]], device='cuda:0')\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0266, 0.1899, 0.0000, 0.1672, 0.3815, 0.0000, 0.0000,\n",
      "          0.1486, 0.0000, 0.0469, 0.0684, 0.0000, 0.0777, 0.2433, 0.0000,\n",
      "          0.4922, 0.6348, 0.0000, 0.6890, 0.5647, 0.0000],\n",
      "         [0.6737, 0.6052, 0.0000, 0.3091, 0.3682, 0.0000, 0.4158, 0.5069,\n",
      "          0.0000, 0.4653, 0.4116, 0.0000, 0.3353, 0.4445, 0.0000, 0.5578,\n",
      "          0.3563, 0.0000, 0.5856, 0.5028, 0.0000, 0.4397, 0.3667, 0.0000,\n",
      "          0.8243, 0.4575, 0.0000, 0.6735, 0.7884, 0.0000, 0.8623, 0.6721,\n",
      "          0.0000, 0.8348, 0.7168, 0.0000, 0.5055, 0.8208],\n",
      "         [0.0000, 0.6990, 0.6798, 0.0000, 0.3753, 0.7778, 0.0000, 0.2987,\n",
      "          0.5772, 0.0000, 0.5215, 0.6983, 0.0000, 0.5512, 0.6159, 0.0000,\n",
      "          0.5158, 0.5041, 0.0000, 0.6662, 0.6010, 0.0000, 0.7650, 0.5699,\n",
      "          0.0000, 0.8128, 0.4305, 0.0000, 0.6946, 0.3823, 0.0000, 1.0953,\n",
      "          0.5249, 0.0000, 1.2180, 0.5322, 0.0000, 1.1452]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "tensor([[1, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0,\n",
      "         0, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 2]], device='cuda:0')\n",
      "Train accuracy: 28.09219287107863\n",
      "Loss: 6054.099516809685\n",
      "\n",
      "Epoch: 7\n",
      "tensor([[1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2,\n",
      "         1, 2, 2, 1, 1, 2, 1, 2, 0, 1, 2, 0, 1, 2]], device='cuda:0')\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0448, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1197,\n",
      "          0.0000, 0.2731, 0.3190, 0.0000, 0.3536, 0.1154, 0.0000, 0.0865,\n",
      "          0.2136, 0.0000, 0.1372, 0.3440, 0.0000, 0.0930, 0.2849, 0.0000,\n",
      "          0.7465, 0.6204, 0.0000, 0.6630, 0.5789, 0.0000],\n",
      "         [0.6051, 0.6592, 0.0000, 0.4920, 0.5256, 0.0000, 0.5772, 0.4262,\n",
      "          0.0000, 0.3885, 0.4342, 0.0000, 0.6717, 0.6071, 0.0000, 0.5469,\n",
      "          0.4448, 0.0000, 0.7848, 0.5721, 0.0000, 0.3980, 0.7217, 0.0000,\n",
      "          0.8740, 0.5356, 0.0000, 0.7642, 0.6122, 0.0000, 0.8096, 0.7043,\n",
      "          0.0000, 0.8102, 0.7276, 0.0000, 0.9359, 0.6331],\n",
      "         [0.0000, 0.7224, 0.7847, 0.0000, 0.6584, 0.5105, 0.0000, 0.3573,\n",
      "          0.6389, 0.0000, 0.4708, 0.3683, 0.0000, 0.7203, 0.6645, 0.0000,\n",
      "          0.5024, 0.5562, 0.0000, 0.6707, 0.6526, 0.0000, 0.5702, 0.5551,\n",
      "          0.0000, 0.9668, 0.5250, 0.0000, 0.5958, 0.4249, 0.0000, 1.3212,\n",
      "          0.3198, 0.0000, 1.3129, 0.4124, 0.0000, 1.5943]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "tensor([[1, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0,\n",
      "         0, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 2]], device='cuda:0')\n",
      "Train accuracy: 27.70431389551268\n",
      "Loss: 6031.348599344026\n",
      "\n",
      "Epoch: 8\n",
      "tensor([[1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2,\n",
      "         1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 0, 2]], device='cuda:0')\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2036,\n",
      "          0.0000, 0.2611, 0.3140, 0.0000, 0.1738, 0.4756, 0.0000, 0.2441,\n",
      "          0.5470, 0.0000, 0.1511, 0.4317, 0.0000, 0.1698, 0.3725, 0.0000,\n",
      "          0.4748, 0.6669, 0.0000, 0.6110, 0.7146, 0.0000],\n",
      "         [0.7357, 1.0194, 0.0000, 0.7121, 0.7212, 0.0000, 0.6267, 0.7853,\n",
      "          0.0000, 0.6641, 0.8143, 0.0000, 0.6248, 0.8963, 0.0000, 0.6090,\n",
      "          0.6803, 0.0000, 0.5797, 0.7435, 0.0000, 0.5623, 0.4600, 0.0000,\n",
      "          0.7547, 0.8419, 0.0000, 0.7988, 0.8000, 0.0000, 0.7872, 0.9040,\n",
      "          0.0000, 0.8846, 0.7791, 0.0000, 0.6527, 0.8531],\n",
      "         [0.0000, 0.7309, 0.9388, 0.0000, 0.6128, 0.8296, 0.0000, 0.6120,\n",
      "          0.7614, 0.0000, 0.5611, 0.6464, 0.0000, 0.6465, 0.6320, 0.0000,\n",
      "          0.3938, 0.8928, 0.0000, 0.8299, 0.8018, 0.0000, 0.6721, 0.6064,\n",
      "          0.0000, 0.6357, 0.4595, 0.0000, 0.5448, 0.4449, 0.0000, 0.6978,\n",
      "          0.5440, 0.0000, 1.2662, 0.6332, 0.0000, 1.1365]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "tensor([[1, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0,\n",
      "         0, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 2]], device='cuda:0')\n",
      "Train accuracy: 26.12957170728686\n",
      "Loss: 5987.376642854651\n",
      "\n",
      "Epoch: 9\n",
      "tensor([[1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1,\n",
      "         1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 0, 1, 2]], device='cuda:0')\n",
      "tensor([[[0.0000, 0.0939, 0.0000, 0.0000, 0.1170, 0.0000, 0.0000, 0.2632,\n",
      "          0.1514, 0.0000, 0.2373, 0.0376, 0.1627, 0.5095, 0.3355, 0.4497,\n",
      "          0.5518, 0.5125, 0.5154, 0.5449, 0.4057, 0.6224, 0.6573, 0.5718,\n",
      "          0.9189, 0.7547, 0.5461, 0.8154, 0.6793, 0.5590, 0.8921, 0.8774,\n",
      "          0.9858, 1.0289, 0.9386, 1.0699, 1.0436, 0.9813],\n",
      "         [1.0783, 1.2964, 0.9755, 1.0001, 1.1736, 1.0158, 1.1173, 1.2078,\n",
      "          0.9945, 1.0708, 1.2082, 0.9158, 0.8416, 1.2577, 1.0178, 0.9949,\n",
      "          1.0734, 0.9270, 1.0049, 1.1928, 1.0342, 1.0633, 0.9457, 0.9511,\n",
      "          1.1212, 1.1751, 0.9975, 1.3479, 1.2871, 1.0347, 1.1463, 1.3195,\n",
      "          1.1214, 1.7524, 1.2479, 1.0448, 1.3351, 1.3036],\n",
      "         [0.9619, 1.2261, 1.3178, 0.9830, 1.2605, 1.1444, 0.8003, 0.6578,\n",
      "          1.4814, 1.0465, 1.0675, 1.2373, 0.9840, 1.0348, 1.2770, 0.9679,\n",
      "          0.9350, 1.2106, 1.0684, 1.1972, 1.1049, 0.9264, 0.9588, 0.7681,\n",
      "          0.8157, 1.0048, 1.2072, 0.9568, 1.0867, 0.8107, 0.9094, 1.3639,\n",
      "          0.7930, 0.8403, 1.5802, 0.8273, 0.9492, 1.8205]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "tensor([[1, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0,\n",
      "         0, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 2]], device='cuda:0')\n",
      "Train accuracy: 18.69870861850051\n",
      "Loss: 5785.7039631472435\n",
      "\n",
      "Epoch: 10\n",
      "tensor([[1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1,\n",
      "         1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2]], device='cuda:0')\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0245, 0.2726,\n",
      "          0.2887, 0.2272, 0.4032, 0.3176, 0.2896, 0.4624, 0.5082, 0.8692,\n",
      "          0.9108, 0.9661, 0.8468, 0.8147, 0.7895, 0.9189, 0.8823, 0.9099,\n",
      "          0.9062, 0.8765, 0.6990, 1.0622, 1.0047, 0.9371, 1.2905, 1.2415,\n",
      "          1.2966, 1.2514, 1.1682, 1.1730, 1.3464, 1.3691],\n",
      "         [1.5257, 1.4494, 1.3445, 1.3178, 1.3080, 1.3464, 1.2566, 1.5436,\n",
      "          1.3955, 1.3009, 1.5371, 1.4176, 1.3916, 1.7396, 1.5263, 1.5690,\n",
      "          1.4701, 1.2711, 1.1721, 1.6357, 1.4558, 1.4424, 1.5348, 1.4631,\n",
      "          1.5557, 1.5134, 1.3377, 1.4180, 1.5252, 1.4078, 1.5576, 1.5090,\n",
      "          1.3624, 1.6722, 1.6010, 1.4645, 1.6245, 1.4759],\n",
      "         [1.2499, 1.3768, 1.7046, 1.5692, 1.5630, 1.4024, 1.2552, 1.2738,\n",
      "          1.3521, 1.3200, 1.3966, 1.5626, 1.3981, 1.4575, 1.5826, 1.4200,\n",
      "          1.2993, 1.4987, 1.4575, 1.4274, 1.3124, 1.2228, 1.4427, 1.4388,\n",
      "          1.3752, 1.5963, 1.2033, 1.1278, 1.2577, 1.1119, 1.0846, 1.4208,\n",
      "          1.1071, 1.0647, 1.9206, 0.8020, 0.8137, 1.9017]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "tensor([[1, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0,\n",
      "         0, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 2]], device='cuda:0')\n",
      "Train accuracy: 18.65380446564058\n",
      "Loss: 5743.248708470142\n",
      "\n",
      "Epoch: 11\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 2,\n",
      "         1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2]], device='cuda:0')\n",
      "tensor([[[0.0000, 0.0000, 0.2268, 0.0700, 0.2143, 0.4537, 0.1187, 0.2392,\n",
      "          0.4752, 0.7814, 0.7939, 0.7945, 0.6398, 0.6290, 0.6852, 1.0699,\n",
      "          1.0684, 1.0928, 1.1347, 1.0851, 1.0298, 1.0130, 0.9962, 0.9462,\n",
      "          1.3664, 1.1735, 0.9890, 1.2878, 1.2292, 1.2187, 1.2961, 1.2365,\n",
      "          1.1743, 1.3669, 1.2592, 1.1496, 1.1985, 1.2354],\n",
      "         [1.4229, 1.3093, 1.2102, 1.2286, 1.4348, 1.3381, 1.2625, 1.8335,\n",
      "          1.7115, 1.6851, 1.5197, 1.4566, 1.5069, 1.6621, 1.5540, 1.4724,\n",
      "          1.6222, 1.4648, 1.3756, 1.4487, 1.3366, 1.3225, 1.5006, 1.5145,\n",
      "          1.7393, 1.8746, 1.7810, 1.6813, 1.6866, 1.7116, 2.0835, 1.6788,\n",
      "          1.6546, 1.7921, 1.5372, 1.4440, 1.4265, 1.8605],\n",
      "         [1.7073, 1.5498, 1.7109, 1.6381, 1.6558, 1.7101, 1.6156, 1.4934,\n",
      "          1.7215, 1.5949, 1.5829, 1.9392, 1.7572, 1.6682, 1.6115, 1.4477,\n",
      "          1.4069, 1.4776, 1.4046, 1.4308, 1.5725, 1.4490, 1.5367, 1.7821,\n",
      "          1.6397, 1.7933, 1.6310, 1.4786, 1.5439, 1.5595, 1.4374, 1.8285,\n",
      "          1.3592, 1.3226, 2.2007, 1.1933, 1.1715, 2.5967]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "tensor([[1, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0,\n",
      "         0, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 2]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 19.039360812610326\n",
      "Loss: 5694.4297565100715\n",
      "\n",
      "Epoch: 12\n",
      "tensor([[1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2]], device='cuda:0')\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0763, 0.0847, 0.1395,\n",
      "          0.4546, 0.3366, 0.3934, 0.5233, 0.7346, 0.6971, 0.7654, 1.2398,\n",
      "          1.2314, 1.2017, 1.2992, 1.2759, 1.2557, 1.1598, 1.1177, 1.1025,\n",
      "          1.3087, 1.2168, 1.1485, 1.5698, 1.5264, 1.4072, 1.4940, 1.5133,\n",
      "          1.5212, 1.7483, 1.6312, 1.5868, 1.6475, 1.7311],\n",
      "         [1.8325, 1.6906, 1.5712, 1.5791, 1.7158, 1.6674, 1.5741, 2.0183,\n",
      "          1.9329, 1.8975, 1.9660, 1.8789, 1.7805, 1.9081, 1.7371, 1.7503,\n",
      "          1.8590, 1.7592, 1.6602, 1.9510, 1.8812, 1.8034, 2.0374, 2.0531,\n",
      "          1.9890, 1.9203, 1.8437, 1.8757, 2.0685, 2.1647, 2.4248, 2.1123,\n",
      "          2.0710, 2.1173, 2.1207, 2.0876, 2.1196, 1.8792],\n",
      "         [1.7238, 1.7755, 2.0369, 1.9419, 1.9130, 1.8264, 1.7289, 1.6978,\n",
      "          1.7779, 1.6532, 1.7147, 1.7585, 1.7038, 1.6123, 1.6155, 1.5707,\n",
      "          1.5249, 1.7154, 1.7129, 1.6644, 1.5483, 1.4448, 1.4576, 1.5827,\n",
      "          1.4864, 1.5162, 1.7615, 1.6998, 1.8686, 1.2732, 1.4391, 2.6189,\n",
      "          1.0242, 1.1816, 2.5813, 1.5199, 1.2361, 2.3493]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "tensor([[1, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0,\n",
      "         0, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 2]], device='cuda:0')\n",
      "Train accuracy: 19.343625158712953\n",
      "Loss: 5659.580901186215\n",
      "\n",
      "Epoch: 13\n",
      "tensor([[2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2]], device='cuda:0')\n",
      "tensor([[[0.1690, 0.2119, 0.5332, 0.5902, 0.7438, 0.9889, 0.7526, 0.8345,\n",
      "          0.9657, 0.9391, 0.9205, 0.9768, 1.3203, 1.3490, 1.3801, 1.5390,\n",
      "          1.5322, 1.4838, 1.6456, 1.6044, 1.5763, 1.5116, 1.4809, 1.4977,\n",
      "          1.4410, 1.4059, 1.3520, 2.0143, 1.8420, 1.7485, 1.7556, 1.6611,\n",
      "          1.6656, 1.6623, 1.6844, 1.6104, 1.6825, 1.7106],\n",
      "         [1.7772, 1.8043, 1.7011, 1.5980, 2.1608, 2.0042, 1.8986, 1.9440,\n",
      "          1.8900, 2.0417, 2.1540, 2.0385, 1.9179, 2.4293, 2.1578, 2.0513,\n",
      "          1.9599, 1.8598, 1.7770, 2.1672, 2.0854, 2.0743, 2.2219, 1.9912,\n",
      "          1.8848, 2.2374, 2.0786, 1.9631, 2.2838, 2.2592, 2.3574, 2.1888,\n",
      "          2.1097, 2.2212, 2.2672, 2.2290, 2.4452, 2.2115],\n",
      "         [2.0321, 2.0481, 2.3246, 2.1052, 1.9992, 2.0443, 1.9041, 1.8534,\n",
      "          2.1708, 2.0312, 1.9814, 1.9076, 1.8021, 1.8074, 1.8418, 1.7144,\n",
      "          1.6795, 2.0179, 1.9974, 1.9932, 1.9669, 1.6671, 1.7714, 1.7411,\n",
      "          1.7274, 1.7269, 2.0311, 1.9063, 1.9494, 1.6052, 1.8816, 2.7014,\n",
      "          1.5330, 1.6087, 2.4678, 1.7462, 1.4581, 2.6903]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "tensor([[1, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0,\n",
      "         0, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 2]], device='cuda:0')\n",
      "Train accuracy: 20.057446347279427\n",
      "Loss: 5620.976184305851\n",
      "\n",
      "Epoch: 14\n",
      "tensor([[1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1,\n",
      "         1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2]], device='cuda:0')\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0718, 0.1982, 0.2350,\n",
      "          0.4586, 0.9644, 1.0415, 1.0456, 1.3789, 1.4596, 1.3400, 1.8101,\n",
      "          1.8815, 1.7831, 1.6886, 1.7221, 1.6418, 1.8877, 1.8973, 1.8395,\n",
      "          2.0529, 1.9345, 1.8204, 1.9239, 1.9126, 1.8055, 1.7715, 1.8302,\n",
      "          1.6849, 1.8921, 1.9233, 1.8412, 2.0057, 2.1114],\n",
      "         [2.3232, 2.1922, 2.2204, 2.1550, 1.9723, 1.9263, 1.8998, 2.0256,\n",
      "          2.0466, 2.0502, 2.2523, 2.1112, 2.0154, 1.9709, 1.8698, 1.7425,\n",
      "          2.2349, 2.1743, 1.9779, 2.3033, 2.2358, 2.1493, 2.3477, 2.3612,\n",
      "          2.3019, 2.0941, 2.1298, 2.0719, 2.3977, 2.5721, 2.6425, 2.2470,\n",
      "          2.2897, 2.4619, 1.9844, 1.9701, 2.2352, 2.1854],\n",
      "         [2.0657, 1.9917, 2.2619, 2.1917, 2.3564, 2.3706, 2.3505, 2.3035,\n",
      "          2.3553, 2.3276, 2.2351, 2.1417, 2.0688, 1.8831, 1.9669, 1.9760,\n",
      "          1.9177, 1.9194, 1.9832, 1.9905, 2.2422, 2.0948, 2.3029, 2.0186,\n",
      "          2.0395, 1.9750, 2.2486, 2.1719, 2.1404, 1.7427, 1.9144, 2.3057,\n",
      "          2.0417, 1.9588, 2.8003, 1.6919, 1.3879, 2.3177]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "tensor([[1, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0,\n",
      "         0, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 2]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-dcabde09c0fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-78-46efaf9f9745>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEpoch: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mtb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-46efaf9f9745>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# compute predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-ef3eaa30db63>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 526\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
