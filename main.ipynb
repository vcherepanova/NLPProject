{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.autograd import Variable\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/valeriya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "documents = {}\n",
    "labels = {}\n",
    "p = re.compile(r\"(\\b[-']\\b)|[\\W_]\")\n",
    "\n",
    "#directories\n",
    "dir_valeria = \"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training\"\n",
    "dir_anna = \"/Users/annasotnikova/Downloads/Hulth2003/Training\"\n",
    "\n",
    "\n",
    "# finction opening the data, splitting it using \".\"\n",
    "def open_data(directory):\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".abstr\"):\n",
    "            content = open((\"%s/%s\" % (directory, file)), \"r\").read()\n",
    "            tmp=[]\n",
    "            for s in content.split('.'):\n",
    "                tmp.append( p.sub(lambda m: (m.group(1) if m.group(1) else \" \"), s) )\n",
    "            documents[file.split('.')[0]] = tmp\n",
    "\n",
    "    # open labels        \n",
    "    # open training labels        \n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".uncontr\"):\n",
    "            content = open((\"%s/%s\" % (directory, file)), \"r\").read()\n",
    "            labels[file.split('.')[0]] = content.split(\"; \")\n",
    "            \n",
    "    return documents, labels\n",
    "\n",
    "# tokenization function\n",
    "def tokenize(doc):\n",
    "    tokenized_doc = {}\n",
    "    for num, ctt in doc.items():\n",
    "        tokenized_doc[num] = []\n",
    "        for sentence in ctt:\n",
    "            words = nltk.word_tokenize(sentence.lower())\n",
    "            tokenized_doc[num].append(words)\n",
    "    return tokenized_doc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, labels = open_data(dir_valeria)\n",
    "tokenized_documents = tokenize(documents)\n",
    "tokenized_labels = tokenize(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we preprocess labels: we label each word in each sentence with 2 if it is not a key-phrase word, \n",
    "# with 0 if it is a first word in key-phrase and 1 if it is not a first word in key-phrase.  \n",
    "\n",
    "# function finds index of element in list of lists\n",
    "def index(lab, target):\n",
    "    for i,phr in enumerate(lab):\n",
    "        for j, w in enumerate(phr):\n",
    "            if w == target:\n",
    "                return (j)\n",
    "    return (None, None)\n",
    "\n",
    "\n",
    "def tag_labels(tokenized_documents, tokenized_labels):\n",
    "    # create dictionary of labels associated to words\n",
    "    class_labels = copy.deepcopy(tokenized_documents)\n",
    "    for document in tokenized_documents:\n",
    "        # take one document\n",
    "        text = tokenized_documents[document]\n",
    "        lab = tokenized_labels[document]\n",
    "        lab_flattened = [val for sublist in lab for val in sublist]\n",
    "        for i, sentence in enumerate(text): \n",
    "            for j, word in enumerate(sentence): \n",
    "                is_keyphrase = word in lab_flattened\n",
    "                if is_keyphrase:\n",
    "                    if index(tokenized_labels[document], word) == 0:\n",
    "                        class_labels[document][i][j] = 1\n",
    "                    else:\n",
    "                        class_labels[document][i][j] = 2\n",
    "                else:\n",
    "                    class_labels[document][i][j] = 0\n",
    "    return class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = tag_labels(tokenized_documents, tokenized_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero 97169\n",
      "one 11942\n",
      "two 20053\n"
     ]
    }
   ],
   "source": [
    "lab_flattened =  [val for sublist in class_labels.values() for val in sublist] \n",
    "lab_flattened  = [val for sublist in lab_flattened for val in sublist] \n",
    "print('zero', lab_flattened.count(0))\n",
    "print('one', lab_flattened.count(1))\n",
    "print('two', lab_flattened.count(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOVE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here we download pretrained glove embeddings\n",
    "import numpy as np\n",
    "embeddings = dict()\n",
    "embed_size = 100\n",
    "f = open('glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings[word] = coefs\n",
    "f.close()\n",
    "\n",
    "glove_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our data\n",
    "X = [sent for doc in copy.deepcopy(tokenized_documents).values() for sent in doc if sent!= []]\n",
    "y = [sent for doc in copy.deepcopy(class_labels).values() for sent in doc if sent!=[]]\n",
    "\n",
    "# our vocab: all the words in all abstracts\n",
    "target_vocab = list(set([item for sublist in X for item in sublist]))\n",
    "# dictionary with all words and their indices\n",
    "vocab_ind_dict = dict(zip(target_vocab, range(0, len(target_vocab)))) \n",
    "\n",
    "\n",
    "# create matrix of glove vectors + random vectors for the words which are in vocab but not in glove\n",
    "\n",
    "matrix_len = len(target_vocab)\n",
    "weights_glove = np.zeros((matrix_len, glove_size))\n",
    "\n",
    "for i, word in enumerate(target_vocab):\n",
    "    try: \n",
    "        weights_glove[i] = embeddings[word]\n",
    "    except KeyError:\n",
    "        weights_glove[i] = np.random.normal(scale=0.6, size=(glove_size, ))\n",
    "\n",
    "# replace words in our data with their indices\n",
    "for i, sent in enumerate(X):\n",
    "    for j, word in enumerate(sent):\n",
    "        X[i][j] = vocab_ind_dict[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, nb_layers = 1, nb_lstm_units=150, nb_lin_units=150, embedding_dim=100, batch_size=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.vocab = target_vocab\n",
    "        self.tags = {'first': 1, 'inside': 2, 'no': 0}\n",
    "        self.nb_lstm_layers = nb_layers\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.nb_lin_units = nb_lin_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_tags = len(self.tags)\n",
    "        nb_vocab_words = len(self.vocab)\n",
    "\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=nb_vocab_words,\n",
    "            embedding_dim=self.embedding_dim)\n",
    "        self.word_embedding.load_state_dict({'weight': torch.Tensor(weights_glove)})\n",
    "        self.word_embedding.weight.requires_grad = True\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            num_layers=self.nb_lstm_layers,\n",
    "            batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.drop = torch.nn.Dropout(p=0.25)\n",
    "\n",
    "        # linear layers\n",
    "        self.lin1 = nn.Linear(2*self.nb_lstm_units, self.nb_lin_units)\n",
    "        self.lin2 = nn.Linear(self.nb_lstm_units, self.nb_tags)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden_a = torch.randn(2, self.batch_size, self.nb_lstm_units).to(device)\n",
    "        hidden_b = torch.randn(2, self.batch_size, self.nb_lstm_units).to(device)\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "\n",
    "        return (hidden_a, hidden_b)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "        X = self.word_embedding(X)\n",
    "        batch_size, seq_len, _ = X.size()\n",
    "\n",
    "        X, self.hidden = self.lstm(X, self.hidden)\n",
    "        X = self.drop(X)\n",
    "        X = X.contiguous()\n",
    "        X = X.view(-1, X.shape[2])\n",
    "\n",
    "        X = F.relu(self.lin1(X))\n",
    "        X = F.relu(self.lin2(X))\n",
    "\n",
    "        X = X.view(batch_size, self.nb_tags, -1)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = LSTM().to(device)\n",
    "weight = torch.tensor([1/8, 1, 1/2]).to(device)\n",
    "criterion= torch.nn.CrossEntropyLoss(weight=weight)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "tb = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training function should be here \n",
    "# Something like \n",
    "\n",
    "def train(epoch):\n",
    "    train_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, sent in enumerate(X):\n",
    "        inputs = torch.LongTensor(sent).view([1, len(sent)]).to(device)\n",
    "        labels = torch.LongTensor(y[i]).view([1, len(y[i])]).to(device)     \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # compute predictions\n",
    "        outputs = (net(inputs))\n",
    "        # compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # statistics to display\n",
    "        train_loss += loss.item()\n",
    "        _,predicted = outputs.max(1)\n",
    "        total += labels[0].size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    print('Train accuracy:', acc)\n",
    "    print('Loss:', train_loss)\n",
    "    tb.add_scalar('Loss', train_loss, epoch)\n",
    "    tb.add_scalar('Accuracy', acc, epoch)\n",
    "    print(predicted)\n",
    "    return(train_loss)\n",
    "\n",
    "\n",
    "def main(num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('\\nEpoch: %d' % epoch)\n",
    "        train_loss = train(epoch)\n",
    "    tb.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    }
   ],
   "source": [
    "main(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
