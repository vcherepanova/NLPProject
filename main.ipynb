{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.autograd import Variable\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/valeriya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Here we download the data and split it into words, still need to get rid of punctuation\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "documents = {}\n",
    "labels = {}\n",
    "\n",
    "# open main data\n",
    "for file in os.listdir(\"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training\"):\n",
    "    if file.endswith(\".abstr\"):\n",
    "        content = open((\"%s/%s\" % ('/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training', file)), \"r\").read()\n",
    "        documents[file.split('.')[0]] = content.split('. ')\n",
    "        \n",
    "# open labels        \n",
    "for file in os.listdir(\"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training\"):\n",
    "    if file.endswith(\".uncontr\"):\n",
    "        content = open((\"%s/%s\" % ('/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training', file)), \"r\").read()\n",
    "        labels[file.split('.')[0]] = content.split(\"; \")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize document\n",
    "tokenized_documents = {}\n",
    "for num, ctt in documents.items():\n",
    "    tokenized_documents[num] = []\n",
    "    for sentence in ctt:\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        tokenized_documents[num].append(words)\n",
    "\n",
    "# tokenize labels\n",
    "tokenized_labels = {}\n",
    "for num, ctt in labels.items():\n",
    "    tokenized_labels[num] = []\n",
    "    for sentence in ctt:\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        tokenized_labels[num].append(words)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we preprocess labels: we label each word in each sentence with \"no\" if it is not a key-phrase word, \n",
    "# with \"first\" if it is a first word in key-phrase and \"inside\" if it is not a first word in key-phrase.  \n",
    "\n",
    "# function finds index of element in list of lists\n",
    "def index(lab, target):\n",
    "    for i,phr in enumerate(lab):\n",
    "        for j, w in enumerate(phr):\n",
    "            if w == target:\n",
    "                return (j)\n",
    "    return (None, None)\n",
    "\n",
    "# create dictionary of labels associated to words\n",
    "class_labels = copy.deepcopy(tokenized_documents)\n",
    "for document in tokenized_documents:\n",
    "    text = tokenized_documents[document]\n",
    "    lab = tokenized_labels[document]\n",
    "    lab_flattened = [val for sublist in lab for val in sublist]\n",
    "    for i, sentence in enumerate(text): \n",
    "        for j, word in enumerate(sentence): \n",
    "            is_keyphrase = word in lab_flattened\n",
    "            if is_keyphrase:\n",
    "                if index(tokenized_labels[document], word) == 0:\n",
    "                    class_labels[document][i][j] = 1\n",
    "                else:\n",
    "                    class_labels[document][i][j] = 2\n",
    "            else:\n",
    "                class_labels[document][i][j] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOVE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here we download pretrained glove embeddings\n",
    "import numpy as np\n",
    "embeddings = dict()\n",
    "embed_size = 100\n",
    "f = open('glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we pad our data: take maximum length of sentence (l) and if length of current sentence (n) is less => \n",
    "# we add word \"PAD\" l-n times to make all sentences having the same length\n",
    "\n",
    "# our data\n",
    "X = [sent for doc in copy.deepcopy(tokenized_documents).values() for sent in doc]\n",
    "y = [sent for doc in copy.deepcopy(class_labels).values() for sent in doc]\n",
    "\n",
    "# padding the data\n",
    "def Padding(data, ext):\n",
    "    len_max = len(max(X, key=lambda coll: len(coll)))\n",
    "    for i, sentence in enumerate(data):\n",
    "        len_sent = len(sentence)\n",
    "        len_pad = len_max - len_sent\n",
    "        sentence.extend([ext for i in range(len_pad)])\n",
    "    return(data)\n",
    "\n",
    "X_padded = Padding(X, 'PAD')\n",
    "y_padded = Padding(y, 0)\n",
    "X_lengths = len(X_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our vocab: all the words in all abstracts\n",
    "target_vocab = set([item for sublist in X_padded for item in sublist])\n",
    "\n",
    "# create matrix of glove vectors + random vectors for the words which are in vocab but not in glove\n",
    "# this matrix is used in the embedding layer\n",
    "\n",
    "matrix_len = len(target_vocab)\n",
    "weights_glove = np.zeros((matrix_len, 100))\n",
    "\n",
    "for i, word in enumerate(target_vocab):\n",
    "    try: \n",
    "        weights_glove[i] = embeddings[word]\n",
    "    except KeyError:\n",
    "        weights_glove[i] = np.random.normal(scale=0.6, size=(100, ))\n",
    "\n",
    "# find which vector corresponds to \"PAD\":\n",
    "pad_idx = list(target_vocab).index('PAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here we replace words with their indices\n",
    "for i, sent in enumerate(X_padded):\n",
    "    for j, word in enumerate(sent):\n",
    "        X_padded[i][j] = list(target_vocab).index(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, nb_layers = 1, nb_lstm_units=150, nb_lin_units=150, embedding_dim=100, batch_size=1,\n",
    "                 pad_idx = pad_idx):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.vocab = target_vocab\n",
    "        self.tags = {'PAD': 0, 'first': 1, 'inside': 2, 'no': 3}\n",
    "        \n",
    "        self.nb_lstm_layers = nb_layers\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.nb_lin_units = nb_lin_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # don't count the padding tag for the classifier output\n",
    "        self.nb_tags = len(self.tags) - 1\n",
    "\n",
    "\n",
    "        nb_vocab_words = len(self.vocab)\n",
    "\n",
    "        # whenever the embedding sees the padding index it'll make the whole vector zeros\n",
    "        padding_idx = pad_idx\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=nb_vocab_words,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        \n",
    "        self.word_embedding.load_state_dict({'weight': torch.Tensor(weights_glove)})\n",
    "        self.word_embedding.weight.requires_grad = False\n",
    "\n",
    "        # design LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            num_layers=self.nb_lstm_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # output layer which projects back to tag space\n",
    "        self.lin1 = nn.Linear(self.nb_lstm_units, self.nb_lin_units)\n",
    "        self.lin2 = nn.Linear(self.nb_lstm_units, self.nb_tags)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden_a = torch.randn(self.nb_lstm_layers, self.batch_size, self.nb_lstm_units).to(device)\n",
    "        hidden_b = torch.randn(self.nb_lstm_layers, self.batch_size, self.nb_lstm_units).to(device)\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "\n",
    "        return (hidden_a, hidden_b)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # reset the LSTM hidden state. Must be done before you run a new batch. Otherwise the LSTM will treat\n",
    "        # a new batch as a continuation of a sequence\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        X_len = torch.LongTensor([43]).to(device) # need to compute  = list of sentence lengths before padding, for ex [8,4,6]\n",
    "\n",
    "        # 1. embed the input\n",
    "        X = self.word_embedding(X)\n",
    "        batch_size, seq_len, _ = X.size()\n",
    "\n",
    "        # 2. Run through RNN\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        X = torch.nn.utils.rnn.pack_padded_sequence(X, X_len, batch_first=True)\n",
    "\n",
    "        # now run through LSTM\n",
    "        X, self.hidden = self.lstm(X, self.hidden)\n",
    "\n",
    "        # undo the packing operation\n",
    "        X, _ = torch.nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "\n",
    "        # 3. Project to tag space\n",
    "        # this one is a bit tricky as well. First we need to reshape the data so it goes into the linear layer\n",
    "        X = X.contiguous()\n",
    "        X = X.view(-1, X.shape[2])\n",
    "\n",
    "        # run through actual linear layer\n",
    "        X = F.relu(self.lin1(X))\n",
    "        X = F.relu(self.lin2(X))\n",
    "\n",
    "        # I like to reshape for mental sanity so we're back to (batch_size, seq_len, nb_tags)\n",
    "        X = X.view(batch_size, self.nb_tags, X_len)\n",
    "\n",
    "        Y_hat = X\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LSTM().to(device)\n",
    "# here we define our loss function and optimizer\n",
    "criterion= torch.nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training function should be here \n",
    "# Something like \n",
    "\n",
    "def train(epoch):\n",
    "    train_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, sent in enumerate(X_padded):\n",
    "        # get sequence of inputs and labels from the data\n",
    "        inputs = torch.LongTensor(sent).view([1, 125]).to(device)\n",
    "        labels = torch.LongTensor(y_padded[i]).view([1, 125]).to(device)     \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # compute predictions\n",
    "        outputs = (net(inputs))\n",
    "        print(outputs)\n",
    "        print(labels)\n",
    "        # compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # statistics to display\n",
    "        train_loss += loss.item()\n",
    "        predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    print('Train accuracy:', acc)\n",
    "    tb.add_scalar('Loss', train_loss, epoch)\n",
    "    tb.add_scalar('Accuracy', acc, epoch)\n",
    "    print(net.conv1.bias.grad.norm())\n",
    "    return(train_loss)\n",
    "\n",
    "\n",
    "def main(num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('\\nEpoch: %d' % epoch)\n",
    "        train_loss = train(epoch)\n",
    "        print(train_loss)\n",
    "        test_accuracy = test()\n",
    "    tb.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 3, 125]' is invalid for input of size 129",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-dcabde09c0fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-e98a9389f931>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEpoch: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-e98a9389f931>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# compute predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-c8bf41f5d356>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# I like to reshape for mental sanity so we're back to (batch_size, seq_len, nb_tags)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m125\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mY_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 3, 125]' is invalid for input of size 129"
     ]
    }
   ],
   "source": [
    "main(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
