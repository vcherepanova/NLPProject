{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword-Extraction using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use BERT Token Classification Model to extract keyword tokens from a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset for BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/Users/dunbanghe/Desktop/BERT/train\"\n",
    "validation_path = \"/Users/dunbanghe/Desktop/BERT/validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_txt = sorted([f for f in os.listdir(train_path) if f.endswith(\".abstr\")])\n",
    "tr_key = sorted([f for f in os.listdir(train_path) if f.endswith(\".uncontr\")])\n",
    "\n",
    "val_txt = sorted([f for f in os.listdir(validation_path) if f.endswith(\".abstr\")])\n",
    "val_key = sorted([f for f in os.listdir(validation_path) if f.endswith(\".uncontr\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_filekey = dict()\n",
    "for i, k in enumerate(tr_txt):\n",
    "    tr_filekey[tr_key[i]] = k\n",
    "\n",
    "val_filekey = dict()\n",
    "for i, k in enumerate(val_txt):\n",
    "    val_filekey[val_key[i]] = k\n",
    "\n",
    "#filekey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(key, filekey, path):\n",
    "    sentences = \"\"\n",
    "    \n",
    "    for line in open(path + \"/\" + filekey[key], 'r'):\n",
    "        sentences += (\" \" + line.rstrip())\n",
    "        sentences = re.sub(r'\\t', '', sentences)\n",
    "        \n",
    "    tokens = sent_tokenize(sentences)\n",
    "    #print(tokens)\n",
    "    \n",
    "    key_file = open(path + \"/\" + str(key),'r').read().rstrip()\n",
    "    key_file = re.sub('\\s+',' ',key_file).lower()\n",
    "    #keys = [line.strip() for line in key_file]\n",
    "    keys = key_file.split(\"; \")\n",
    "    #print(keys)\n",
    "    \n",
    "    key_sent = []\n",
    "    labels = []\n",
    "    for token in tokens:\n",
    "        z = ['O'] * len(token.split())\n",
    "        for k in keys:\n",
    "            if k in token:\n",
    "                \n",
    "                if len(k.split())==1:\n",
    "                    try:\n",
    "                        z[token.lower().split().index(k.lower().split()[0])] = 'B'\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                elif len(k.split())>1:\n",
    "                    try:\n",
    "                        if token.lower().split().index(k.lower().split()[0]) and token.lower().split().index(k.lower().split()[-1]):\n",
    "                            z[token.lower().split().index(k.lower().split()[0])] = 'B'\n",
    "                            for j in range(1, len(k.split())):\n",
    "                                z[token.lower().split().index(k.lower().split()[j])] = 'I'\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        for m, n in enumerate(z):\n",
    "            if z[m] == 'I' and z[m-1] == 'O':\n",
    "                z[m] = 'O'\n",
    "\n",
    "        if set(z) != {'O'}:\n",
    "            labels.append(z) \n",
    "            key_sent.append(token)\n",
    "    return key_sent, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3134 3134\n",
      "1452 1452\n"
     ]
    }
   ],
   "source": [
    "tr_sentences_ = []\n",
    "tr_labels_ = []\n",
    "val_sentences_ = []\n",
    "val_labels_ = []\n",
    "\n",
    "for tr_key, tr_value in tr_filekey.items():\n",
    "    s, l = convert(tr_key,tr_filekey,train_path)\n",
    "    tr_sentences_.append(s)\n",
    "    tr_labels_.append(l)\n",
    "tr_sentences = [item for sublist in tr_sentences_ for item in sublist]\n",
    "tr_labels = [item for sublist in tr_labels_ for item in sublist]\n",
    "\n",
    "#print(tr_sentences)\n",
    "print(len(tr_sentences), len(tr_labels))\n",
    "\n",
    "\n",
    "for val_key, val_value in val_filekey.items():\n",
    "    s, l = convert(val_key,val_filekey,validation_path)\n",
    "    val_sentences_.append(s)\n",
    "    val_labels_.append(l)\n",
    "val_sentences = [item for sublist in val_sentences_ for item in sublist]\n",
    "val_labels = [item for sublist in val_labels_ for item in sublist]\n",
    "\n",
    "print(len(val_sentences), len(val_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 75\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx = {'B': 0, 'I': 1, 'O': 2}\n",
    "tags_vals = ['B', 'I', 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1452"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts_tr = [tokenizer.tokenize(sent) for sent in tr_sentences]\n",
    "tokenized_texts_val = [tokenizer.tokenize(sent) for sent in val_sentences]\n",
    "#len(tokenized_texts_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_tr],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "tr_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in tr_labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "val_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_val],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "val_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in val_labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_attention_masks = [[float(i>0) for i in ii] for ii in tr_input_ids]\n",
    "val_attention_masks = [[float(i>0) for i in ii] for ii in val_input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1452\n"
     ]
    }
   ],
   "source": [
    "#tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(tr_input_ids, tr_tags, random_state=2018, test_size=0.1)\n",
    "#tr_masks, val_masks, _, _ = train_test_split(tr_attention_masks, tr_input_ids, random_state=2018, test_size=0.1)\n",
    "\n",
    "tr_inputs = tr_input_ids\n",
    "tr_tags = tr_tags\n",
    "tr_masks = tr_attention_masks\n",
    "\n",
    "val_inputs = val_input_ids\n",
    "#print(len(val_inputs))\n",
    "val_tags = val_tags\n",
    "val_masks = val_attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.36790759040384874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  25%|██▌       | 1/4 [22:47<1:08:23, 1367.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3278063898501189\n",
      "Train loss: 0.29789082067353384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 2/4 [48:05<47:05, 1412.72s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.30948931270319485\n",
      "Train loss: 0.25308880498822856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  75%|███████▌  | 3/4 [1:12:39<23:51, 1431.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.28731507872757706\n",
      "Train loss: 0.2121264496628119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 4/4 [1:41:25<00:00, 1521.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2890006410686866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        loss = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    torch.save(model.state_dict(), 'checkpoint.pth')\n",
    "    \n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                                  attention_mask=b_input_mask, labels=b_labels)\n",
    "            logits = model(b_input_ids, token_type_ids=None,\n",
    "                           attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    #print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    #print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pred_2.txt', 'w') as f:\n",
    "    print(pred_tags, file=f)\n",
    "with open('true_2.txt', 'w') as f:\n",
    "    print(valid_tags, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9390126811594202\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def recall(kp_true, kp_predicted):\n",
    "    tp = 0\n",
    "    fn = 0 \n",
    "    for i in kp_true:\n",
    "        if (i in kp_predicted):\n",
    "            tp = tp + 1\n",
    "        else:\n",
    "            fn = fn + 1\n",
    "    return float(tp)/(float(tp)+float(fn)) if (float(tp)+float(fn))!=0 else 0\n",
    "\n",
    "\n",
    "def precision(kp_true, kp_predicted):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for i in kp_predicted:\n",
    "        if(i in kp_true):\n",
    "            tp = tp + 1\n",
    "        else:\n",
    "            fp  = fp + 1\n",
    "    \n",
    "    return float(tp)/(float(tp)+float(fp))  if (float(tp)+float(fp)) > 0 else 0\n",
    "\n",
    "def f1(kp_true, kp_predicted):\n",
    "    precision_ = precision(kp_true, kp_predicted)\n",
    "    recall_ = recall(kp_true, kp_predicted)\n",
    "    return (2 * (precision_ * recall_)) / (precision_ + recall_) if precision_ + recall_ > 0 else 0\n",
    "\n",
    "\n",
    "def retrive_phrase(tags_predicted, document_eng):\n",
    "    #from tag 0,1,2 to keyphrase arrays (2d-sentences, words)\n",
    "    kp = []\n",
    "    sentence= []\n",
    "    for i in range(len(tags_predicted)):\n",
    "        if (tags_predicted[i] == 'O'):\n",
    "            if len(sentence) != 0:\n",
    "                kp.append(copy.deepcopy(sentence))\n",
    "                \n",
    "            sentence.clear()#we know a sentence ends when we encounter 0, so push\n",
    "        elif((tags_predicted[i] == 'B')):\n",
    "            if len(sentence) != 0:\n",
    "                kp.append(copy.deepcopy(sentence))\n",
    "            sentence.clear()#we know a sentence ends when we encounter 1, so push\n",
    " \n",
    "            sentence.append(document_eng[i])\n",
    "            if(i== len(tags_predicted)-1):#if it is the last element, we push\n",
    "                kp.append(copy.deepcopy(sentence))\n",
    "        else:\n",
    "            sentence.append(document_eng[i])\n",
    "            if(i== len(tags_predicted)-1):\n",
    "                kp.append(copy.deepcopy(sentence))\n",
    "    return kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(pred_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(tokenized_texts_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import operator\n",
    "#from functools import reduce\n",
    "#len(reduce(operator.add, tokenized_texts_val))\n",
    "#len(reduce(operator.add, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrive_phrase(pred,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
