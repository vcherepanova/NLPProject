{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from keras import regularizers\n",
    "from keras.layers import Bidirectional, Dense, Dropout, Embedding, LSTM, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "from data_preprocessing import open_data, tokenize, tag_document, data_to_seq, glove_emb_matrix, tags_to_3D, clean_data\n",
    "from validation import precision, recall, f1, retrive_phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/annasotnikova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "documents = {}\n",
    "labels = {}\n",
    "\n",
    "#directories\n",
    "dir_Tu= \"/Users/kmirai/Downloads/NLPProject-master/Hulth2003/Training\"\n",
    "\n",
    "dir_valeria_train = \"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training\"\n",
    "dir_valeria_val = \"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Validation\"\n",
    "dir_valeria_test = \"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Test\"\n",
    "\n",
    "dir_anna_train = \"/Users/annasotnikova/Downloads/Hulth2003/Training\"\n",
    "dir_anna_val = \"/Users/annasotnikova/Downloads/Hulth2003/Validation\"\n",
    "dir_anna_test = \"/Users/annasotnikova/Downloads/Hulth2003/Test\"\n",
    "\n",
    "\n",
    "\n",
    "dir_anna_s_train = \"/Users/annasotnikova/Downloads/NLPProject-master-6/Semeval2017\"\n",
    "dir_anna_s_test = \"/Users/annasotnikova/Downloads/NLPProject-master-6/Semeval2017\"\n",
    "dir_anna_s_dev = \"/Users/annasotnikova/Downloads/NLPProject-master-6/Semeval2017/dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#open data\n",
    "documents_train, labels_train = open_data(dir_anna_train)\n",
    "documents_val, labels_val = open_data(dir_anna_val)\n",
    "documents_test, labels_test = open_data(dir_anna_test)\n",
    "\n",
    "# tokenize data\n",
    "tokenized_documents_train, tokenized_labels_train = tokenize(documents_train, labels_train)\n",
    "tokenized_documents_val, tokenized_labels_val = tokenize(documents_val, labels_val)\n",
    "tokenized_documents_test, tokenized_labels_test = tokenize(documents_test, labels_test)\n",
    "\n",
    "# create sequence of labels (tags) for the documents\n",
    "tags_train, tokenized_labels_train = tag_document(tokenized_documents_train, tokenized_labels_train)\n",
    "tags_val, tokenized_labels_val = tag_document(tokenized_documents_val, tokenized_labels_val)\n",
    "tags_test, tokenized_labels_test = tag_document(tokenized_documents_test, tokenized_labels_test)\n",
    "\n",
    "# remove documents without keyphrases \n",
    "tokenized_documents_train, tags_train, tokenized_labels_train = clean_data(tokenized_documents_train,\n",
    "                                                                           tags_train, tokenized_labels_train)\n",
    "tokenized_documents_val, tags_val, tokenized_labels_val = clean_data(tokenized_documents_val,\n",
    "                                                                           tags_val, tokenized_labels_val)\n",
    "tokenized_documents_test, tags_test, tokenized_labels_test = clean_data(tokenized_documents_test,\n",
    "                                                                           tags_test, tokenized_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(path,folder):\n",
    "    # This dictionary will contain the documents\n",
    "    documents = {}\n",
    "\n",
    "    for doc in os.listdir(\"%s/%s\" % (path, folder)):\n",
    "        if doc.endswith(\".txt\"):\n",
    "            content = open((\"%s/%s/%s\" % (path, folder, doc)), \"r\").read()\n",
    "            documents[doc[:doc.find('.')]] = content\n",
    "\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(path, folder):\n",
    "        '''\n",
    "        Loads the answers contained in the .ann files\n",
    "        and puts them in a dictionary indexed by document ID\n",
    "        (i.e. the document name without the extension).\n",
    "\n",
    "        Adapted from readAnn() from the official Semeval 2017 Scripts.\n",
    "        '''\n",
    "\n",
    "        answers = {}\n",
    "\n",
    "        file_list = os.listdir(\"%s/%s\" % (path, folder))\n",
    "        for filename in file_list:\n",
    "            if not filename.endswith(\".ann\"):\n",
    "                continue\n",
    "            file_anno = open(os.path.join(\"%s/%s\" % (path, folder), filename), \"rU\")\n",
    "            file_text = open(os.path.join(\"%s/%s\" % (path, folder), filename.replace(\".ann\", \".txt\")), \"rU\")\n",
    "            doc_id = filename[:filename.find('.')]\n",
    "\n",
    "            answers[doc_id] = []\n",
    "\n",
    "            # there's only one line, as each .txt file is one text paragraph\n",
    "            for l in file_text:\n",
    "                text = l\n",
    "\n",
    "            for l in file_anno:\n",
    "                anno_inst = l.strip(\"\\n\").split(\"\\t\")\n",
    "                if len(anno_inst) == 3:\n",
    "                    anno_inst1 = anno_inst[1].split(\" \")\n",
    "                    if len(anno_inst1) == 3:\n",
    "                        keytype, start, end = anno_inst1\n",
    "                    else:\n",
    "                        keytype, start, _, end = anno_inst1\n",
    "                    if not keytype.endswith(\"-of\"):\n",
    "\n",
    "                        # look up span in text and print error message if it doesn't match the .ann span text\n",
    "                        keyphr_text_lookup = text[int(start):int(end)]\n",
    "                        keyphr_ann = anno_inst[2]\n",
    "                        if keyphr_text_lookup != keyphr_ann:\n",
    "                            logging.warning(\"Spans don't match for anno %s in file %s\" % (l.strip(), filename))\n",
    "                        else:\n",
    "                            answers[doc_id].append(keyphr_ann)\n",
    "\n",
    "        return answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: 'U' mode is deprecated\n",
      "  app.launch_new_instance()\n",
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: 'U' mode is deprecated\n"
     ]
    }
   ],
   "source": [
    "#open data semeval2017\n",
    "documents_train_s = load_doc(dir_anna_s_train, \"train\")\n",
    "labels_train_s = load_labels(dir_anna_s_train, \"train\")\n",
    "documents_val_s = load_doc(dir_anna_s_dev, \"dev\")\n",
    "labels_val_s = load_labels(dir_anna_s_dev, \"dev\")\n",
    "documents_test_s = load_doc(dir_anna_s_test, \"test\")\n",
    "labels_test_s = load_labels(dir_anna_s_test, \"test\")\n",
    "\n",
    "# tokenize data semeval2017\n",
    "tokenized_documents_train_s, tokenized_labels_train_s = tokenize(documents_train_s, labels_train_s)\n",
    "tokenized_documents_val_s, tokenized_labels_val_s = tokenize(documents_val_s, labels_val_s)\n",
    "tokenized_documents_test_s, tokenized_labels_test_s = tokenize(documents_test_s, labels_test_s)\n",
    "\n",
    "# create sequence of labels (tags) for the documents\n",
    "tags_train_s, tokenized_labels_train_s = tag_document(tokenized_documents_train_s, tokenized_labels_train_s)\n",
    "tags_val_s, tokenized_labels_val_s = tag_document(tokenized_documents_val_s, tokenized_labels_val_s)\n",
    "tags_test_s, tokenized_labels_test_s = tag_document(tokenized_documents_test_s, tokenized_labels_test_s)\n",
    "\n",
    "# remove documents without keyphrases \n",
    "tokenized_documents_train_s, tags_train_s, tokenized_labels_train_s = clean_data(tokenized_documents_train_s,\n",
    "                                                                           tags_train_s, tokenized_labels_train_s)\n",
    "tokenized_documents_val_s, tags_val_s, tokenized_labels_val_s = clean_data(tokenized_documents_val_s,\n",
    "                                                                           tags_val_s, tokenized_labels_val_s)\n",
    "tokenized_documents_test_s, tags_test_s, tokenized_labels_test_s = clean_data(tokenized_documents_test_s,\n",
    "                                                                           tags_test_s, tokenized_labels_test_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOVE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = dict()\n",
    "embed_size = 100\n",
    "f = open('glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove[word] = coefs\n",
    "f.close()\n",
    "\n",
    "glove_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary from all data \n",
    "X_train_eng = [doc for doc in copy.deepcopy(tokenized_documents_train_s).values()]\n",
    "X_val_eng = [doc for doc in copy.deepcopy(tokenized_documents_val_s).values()]\n",
    "X_test_eng = [doc for doc in copy.deepcopy(tokenized_documents_test_s).values()]\n",
    "X_full = X_train_eng + X_val_eng + X_test_eng\n",
    "\n",
    "# Our vocab: all the words in all abstracts\n",
    "target_vocab = list(set([token for doc in X_full for token in doc]))\n",
    "# Dictionary with all words and their indices\n",
    "vocab_ind_dict = dict(zip(target_vocab, range(0, len(target_vocab)))) \n",
    "# Embedding matrix\n",
    "embed_matrix = glove_emb_matrix(vocab_ind_dict, glove, glove_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for network\n",
    "X_train = data_to_seq(X_train_eng, vocab_ind_dict)\n",
    "X_val = data_to_seq(X_val_eng, vocab_ind_dict)\n",
    "X_test = data_to_seq(X_test_eng, vocab_ind_dict)\n",
    "\n",
    "kp_train = [doc for doc in copy.deepcopy(tokenized_labels_train_s).values()]\n",
    "tags_train = [doc for doc in copy.deepcopy(tags_train_s).values()]\n",
    "kp_val = [doc for doc in copy.deepcopy(tokenized_labels_val_s).values()]\n",
    "tags_val = [doc for doc in copy.deepcopy(tags_val_s).values()]\n",
    "kp_test = [doc for doc in copy.deepcopy(tokenized_labels_test_s).values()]\n",
    "tags_test = [doc for doc in copy.deepcopy(tags_test_s).values()]\n",
    "\n",
    "# Padding \n",
    "X_train_padded = pad_sequences(X_train, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "X_val_padded = pad_sequences(X_val, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "X_test_padded = pad_sequences(X_test, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "\n",
    "tags_train_padded = pad_sequences(tags_train, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "tags_val_padded = pad_sequences(tags_val, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "tags_test_padded = pad_sequences(tags_test, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "\n",
    "# Convert labels to 3D as keras likes\n",
    "tags_train_3d = tags_to_3D(tags_train_padded)\n",
    "tags_val_3d = tags_to_3D(tags_val_padded)\n",
    "tags_test_3d = tags_to_3D(tags_test_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.reshape(class_weight.compute_sample_weight('balanced', tags_train_padded.flatten()),\n",
    "                             np.shape(tags_train_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 550, 100)          1202000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 550, 600)          962400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 550, 600)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 550, 150)          90150     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 550, 150)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 550, 3)            453       \n",
      "=================================================================\n",
      "Total params: 2,255,003\n",
      "Trainable params: 1,053,003\n",
      "Non-trainable params: 1,202,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 350 samples, validate on 50 samples\n",
      "Epoch 1/8\n",
      "  8/350 [..............................] - ETA: 4:12 - loss: 3.4398 - accuracy: 0.4482"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-314e424a6204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     sample_weight=weights)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_SIZE = 100\n",
    "MAX_DOCUMENT_LENGTH = 550\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 8\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(np.shape(embed_matrix)[0],\n",
    "                            EMBEDDINGS_SIZE,\n",
    "                            weights=[embed_matrix],\n",
    "                            input_length=MAX_DOCUMENT_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(LSTM(300, activation='tanh', recurrent_activation='hard_sigmoid', return_sequences=True)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(TimeDistributed(Dense(150, activation='relu', kernel_regularizer=regularizers.l2(0.01))))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(TimeDistributed(Dense(3, activation='softmax')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'],\n",
    "              sample_weight_mode=\"temporal\")\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "history = model.fit(X_train_padded, tags_train_3d,\n",
    "                    validation_data=(X_val_padded, tags_val_3d),\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    sample_weight=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(documents_eng, kp_eng, documents_seq, tags, model):\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    f_score = 0\n",
    "    acc = 0\n",
    "    predictions = model.predict_classes(documents_seq)\n",
    "    for idx, document_eng in enumerate(documents_eng):\n",
    "        # our document (unpadding)\n",
    "        doc_len = len(documents_eng[idx])\n",
    "        document_seq = documents_seq[idx][0:doc_len]\n",
    "        tags_predicted = predictions[idx][0:doc_len]\n",
    "        # predicted kp\n",
    "        kp_predicted = retrive_phrase(tags_predicted, document_eng)\n",
    "        kp_true = kp_eng[idx]\n",
    "        tags_true = tags[idx]\n",
    "        # compute precision, recall, f_score, accuracy\n",
    "        prec += precision(kp_true, kp_predicted)\n",
    "        rec += recall(kp_true, kp_predicted)\n",
    "        f_score += f1(kp_true, kp_predicted)\n",
    "        acc += sum(np.equal(tags_true, tags_predicted))/len(tags_true)\n",
    "        #if idx == 1:\n",
    "        #    print('document_eng', document_eng)\n",
    "        #    print('document_seq', document_seq)\n",
    "        #    print(\"kp_true\",kp_true)\n",
    "        #    print(\"tags_true\" ,tags_true)\n",
    "        #    print(\"tags_predicted\", tags_predicted)\n",
    "        #    print(\"kp_predicted\", kp_predicted)\n",
    "    return prec/len(documents_eng), rec/len(documents_eng), f_score/len(documents_eng), acc/len(documents_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr, r, f, acc = validate(X_val_eng, kp_val, X_val_padded, tags_val, model)\n",
    "print('Validation Accuracy', acc)\n",
    "print('Validation Precision', pr)\n",
    "print('Validation Recall', r)\n",
    "print('Validation F-score', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
