{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from keras import regularizers\n",
    "from keras.layers import Bidirectional, Dense, Dropout, Embedding, LSTM, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "from data_preprocessing import open_data, tokenize, tag_document, data_to_seq, glove_emb_matrix, tags_to_3D\n",
    "from validation import precision, recall, f1, retrive_phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/valeriya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "documents = {}\n",
    "labels = {}\n",
    "\n",
    "#directories\n",
    "dir_Tu= \"/Users/kmirai/Downloads/NLPProject-master/Hulth2003/Training\"\n",
    "\n",
    "dir_valeria_train = \"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training\"\n",
    "dir_valeria_val = \"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Validation\"\n",
    "dir_valeria_test = \"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Test\"\n",
    "\n",
    "dir_anna = \"/Users/annasotnikova/Downloads/Hulth2003/Training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#open data\n",
    "documents_train, labels_train = open_data(dir_valeria_train)\n",
    "documents_val, labels_val = open_data(dir_valeria_val)\n",
    "documents_test, labels_test = open_data(dir_valeria_test)\n",
    "\n",
    "# tokenize data\n",
    "tokenized_documents_train, tokenized_labels_train = tokenize(documents_train, labels_train)\n",
    "tokenized_documents_val, tokenized_labels_val = tokenize(documents_val, labels_val)\n",
    "tokenized_documents_test, tokenized_labels_test = tokenize(documents_test, labels_test)\n",
    "\n",
    "# create sequence of labels (tags) for the documents\n",
    "tags_train = tag_document(tokenized_documents_train, tokenized_labels_train)\n",
    "tags_val = tag_document(tokenized_documents_val, tokenized_labels_val)\n",
    "tags_test = tag_document(tokenized_documents_test, tokenized_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOVE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove = dict()\n",
    "embed_size = 100\n",
    "f = open('glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove[word] = coefs\n",
    "f.close()\n",
    "\n",
    "glove_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary from all data \n",
    "X_train_eng = [doc for doc in copy.deepcopy(tokenized_documents_train).values()]\n",
    "X_val_eng = [doc for doc in copy.deepcopy(tokenized_documents_val).values()]\n",
    "X_test_eng = [doc for doc in copy.deepcopy(tokenized_documents_test).values()]\n",
    "X_full = X_train_eng + X_val_eng + X_test_eng\n",
    "\n",
    "# Our vocab: all the words in all abstracts\n",
    "target_vocab = list(set([token for doc in X_full for token in doc]))\n",
    "# Dictionary with all words and their indices\n",
    "vocab_ind_dict = dict(zip(target_vocab, range(0, len(target_vocab)))) \n",
    "# Embedding matrix\n",
    "embed_matrix = glove_emb_matrix(vocab_ind_dict, glove, glove_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for network\n",
    "X_train = data_to_seq(X_train_eng, vocab_ind_dict)\n",
    "X_val = data_to_seq(X_val_eng, vocab_ind_dict)\n",
    "X_test = data_to_seq(X_test_eng, vocab_ind_dict)\n",
    "\n",
    "kp_train = [doc for doc in copy.deepcopy(tokenized_labels_train).values()]\n",
    "tags_train = [doc for doc in copy.deepcopy(tags_train).values()]\n",
    "kp_val = [doc for doc in copy.deepcopy(tokenized_labels_val).values()]\n",
    "tags_val = [doc for doc in copy.deepcopy(tags_val).values()]\n",
    "kp_test = [doc for doc in copy.deepcopy(tokenized_labels_test).values()]\n",
    "tags_test = [doc for doc in copy.deepcopy(tags_test).values()]\n",
    "\n",
    "# Padding \n",
    "X_train_padded = pad_sequences(X_train, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "X_val_padded = pad_sequences(X_val, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "X_test_padded = pad_sequences(X_test, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "\n",
    "tags_train_padded = pad_sequences(tags_train, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "tags_val_padded = pad_sequences(tags_val, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "tags_test_padded = pad_sequences(tags_test, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "\n",
    "# Convert labels to 3D as keras likes\n",
    "tags_train_3d = tags_to_3D(tags_train_padded)\n",
    "tags_val_3d = tags_to_3D(tags_val_padded)\n",
    "tags_test_3d = tags_to_3D(tags_test_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(documents_eng, kp_eng, documents_seq, tags, model):\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    f_score = 0\n",
    "    acc = 0\n",
    "    predictions = model.predict_classes(documents_seq)\n",
    "    \n",
    "    for idx, document_eng in enumerate(documents_eng):\n",
    "        # our document (unpadding)\n",
    "        doc_len = len(documents_eng[idx])\n",
    "        document_seq = documents_seq[idx][0:doc_len]\n",
    "        tags_predicted = predictions[idx][0:doc_len]\n",
    "        # predicted kp\n",
    "        kp_predicted = retrive_phrase(tags_predicted, document_eng)\n",
    "        kp_true = kp_eng[idx]\n",
    "        tags_true = tags[idx]\n",
    "        # compute precision, recall, f_score, accuracy\n",
    "        prec += precision(kp_true, kp_predicted)\n",
    "        rec += recall(kp_true, kp_predicted)\n",
    "        f_score += f1(kp_true, kp_predicted)\n",
    "        acc += sum(np.equal(tags_true, tags_predicted))/len(tags_true)\n",
    "        #if idx == 1:\n",
    "        #    print('document_eng', document_eng)\n",
    "        #    print('document_seq', document_seq)\n",
    "        #    print(\"kp_true\",kp_true)\n",
    "        #    print(\"tags_true\" ,tags_true)\n",
    "        #    print(\"tags_predicted\", tags_predicted)\n",
    "        #    print(\"kp_predicted\", kp_predicted)\n",
    "    return prec/len(documents_eng), rec/len(documents_eng), f_score/len(documents_eng), acc/len(documents_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = np.reshape(class_weight.compute_sample_weight('balanced', tags_train_padded.flatten()),\n",
    "                             np.shape(tags_train_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 550, 100)          1827500   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 550, 600)          962400    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 550, 600)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 550, 150)          90150     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 550, 150)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 550, 3)            453       \n",
      "=================================================================\n",
      "Total params: 2,880,503\n",
      "Trainable params: 1,053,003\n",
      "Non-trainable params: 1,827,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1000 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 229s 229ms/step - loss: 0.7211 - accuracy: 0.8695 - val_loss: 0.2613 - val_accuracy: 0.9182\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 229s 229ms/step - loss: 0.2892 - accuracy: 0.9100 - val_loss: 0.2466 - val_accuracy: 0.9242\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 239s 239ms/step - loss: 0.2626 - accuracy: 0.9166 - val_loss: 0.2428 - val_accuracy: 0.9224\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 238s 238ms/step - loss: 0.2438 - accuracy: 0.9210 - val_loss: 0.2079 - val_accuracy: 0.9330\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 238s 238ms/step - loss: 0.2266 - accuracy: 0.9242 - val_loss: 0.2658 - val_accuracy: 0.9196\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 238s 238ms/step - loss: 0.2097 - accuracy: 0.9276 - val_loss: 0.2012 - val_accuracy: 0.9377\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 232s 232ms/step - loss: 0.1905 - accuracy: 0.9306 - val_loss: 0.2210 - val_accuracy: 0.9297\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 229s 229ms/step - loss: 0.1745 - accuracy: 0.9336 - val_loss: 0.2289 - val_accuracy: 0.9342\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 237s 237ms/step - loss: 0.1576 - accuracy: 0.9372 - val_loss: 0.1976 - val_accuracy: 0.9443\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 240s 240ms/step - loss: 0.1423 - accuracy: 0.9390 - val_loss: 0.2242 - val_accuracy: 0.9378\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_SIZE = 100\n",
    "MAX_DOCUMENT_LENGTH = 550\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(np.shape(embed_matrix)[0],\n",
    "                            EMBEDDINGS_SIZE,\n",
    "                            weights=[embed_matrix],\n",
    "                            input_length=MAX_DOCUMENT_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(LSTM(300, activation='tanh', recurrent_activation='hard_sigmoid', return_sequences=True)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(TimeDistributed(Dense(150, activation='relu', kernel_regularizer=regularizers.l2(0.01))))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(TimeDistributed(Dense(3, activation='softmax')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'],\n",
    "              sample_weight_mode=\"temporal\")\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "history = model.fit(X_train_padded, tags_train_3d,\n",
    "                    validation_data=(X_val_padded, tags_val_3d),\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    sample_weight=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 0.7439477995503742\n",
      "Validation Precision 0.25686407214164225\n",
      "Validation Recall 0.601197251475809\n",
      "Validation F-score 0.3479060476256512\n"
     ]
    }
   ],
   "source": [
    "pr, r, f, acc = validate(X_val_eng, kp_val, X_val_padded, tags_val, model)\n",
    "print('Validation Accuracy', acc)\n",
    "print('Validation Precision', pr)\n",
    "print('Validation Recall', r)\n",
    "print('Validation F-score', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
