{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from keras import regularizers\n",
    "from keras.layers import Bidirectional, Dense, Dropout, Embedding, LSTM, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "from data_preprocessing import open_data, tokenize, tag_document, data_to_seq, glove_emb_matrix, tags_to_2D, clean_data\n",
    "from validation import precision, recall, f1, retrive_phrase_IO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/valeriya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "documents = {}\n",
    "labels = {}\n",
    "\n",
    "#directories\n",
    "dir_Tu= \"/Users/kmirai/Downloads/NLPProject-master/Hulth2003/Training\"\n",
    "\n",
    "dir_valeria_train = \"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training\"\n",
    "dir_valeria_val = \"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Validation\"\n",
    "dir_valeria_test = \"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Test\"\n",
    "\n",
    "dir_anna = \"/Users/annasotnikova/Downloads/Hulth2003/Training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opening training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open data\n",
    "documents_train, labels_train = open_data(dir_valeria_train)\n",
    "documents_val, labels_val = open_data(dir_valeria_val)\n",
    "documents_test, labels_test = open_data(dir_valeria_test)\n",
    "\n",
    "# tokenize data\n",
    "tokenized_documents_train, tokenized_labels_train = tokenize(documents_train, labels_train)\n",
    "tokenized_documents_val, tokenized_labels_val = tokenize(documents_val, labels_val)\n",
    "tokenized_documents_test, tokenized_labels_test = tokenize(documents_test, labels_test)\n",
    "\n",
    "# create sequence of labels (tags) for the documents\n",
    "tags_train, tokenized_labels_train = tag_document(tokenized_documents_train, tokenized_labels_train)\n",
    "tags_val, tokenized_labels_val = tag_document(tokenized_documents_val, tokenized_labels_val)\n",
    "tags_test, tokenized_labels_test = tag_document(tokenized_documents_test, tokenized_labels_test)\n",
    "\n",
    "# remove documents without keyphrases \n",
    "tokenized_documents_train, tags_train, tokenized_labels_train = clean_data(tokenized_documents_train,\n",
    "                                                                           tags_train, tokenized_labels_train)\n",
    "tokenized_documents_val, tags_val, tokenized_labels_val = clean_data(tokenized_documents_val,\n",
    "                                                                           tags_val, tokenized_labels_val)\n",
    "tokenized_documents_test, tags_test, tokenized_labels_test = clean_data(tokenized_documents_test,\n",
    "                                                                           tags_test, tokenized_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want to predict kyphrases for specific example, now is a good time to define it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstr = 'In standard reinforcement learning (RL), a learning agent seeks to optimize the overall reward. However, many key aspects of a desired behavior are more naturally expressed as constraints. For instance, the designer may want to limit the use of unsafe actions, increase the diversity of trajectories to enable exploration, or approximate expert trajectories when rewards are sparse. In this paper, we propose an algorithmic scheme that can handle a wide class of constraints in RL tasks, specifically, any constraints that require expected values of some vector measurements (such as the use of an action) to lie in a convex set. This captures previously studied constraints (such as safety and proximity to an expert), but also enables new classes of constraints (such as diversity). Our approach comes with rigorous theoretical guarantees and only relies on the ability to approximately solve standard RL tasks. As a result, it can be easily adapted to work with any model-free or model-based RL algorithm. In our experiments, we show that it matches previous algorithms that enforce safety via constraints, but can also enforce new properties that these algorithms cannot incorporate, such as diversity.'.lower()\n",
    "tok_abstr = nltk.word_tokenize(abstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting BIO tags to IO tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIO_to_IO(labels):\n",
    "    for key in labels.keys():\n",
    "        for i, token in enumerate(labels[key]):\n",
    "            if token==2:\n",
    "                labels[key][i] = 1\n",
    "    return labels \n",
    "            \n",
    "tags_train = BIO_to_IO(tags_train)\n",
    "tags_val = BIO_to_IO(tags_val)\n",
    "tags_test = BIO_to_IO(tags_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOVE embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spcify desired embedding size, use 100 by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = dict()\n",
    "f = open('glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove[word] = coefs\n",
    "f.close()\n",
    "\n",
    "glove_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary from all data \n",
    "X_train_eng = [doc for doc in copy.deepcopy(tokenized_documents_train).values()]\n",
    "X_val_eng = [doc for doc in copy.deepcopy(tokenized_documents_val).values()]\n",
    "X_test_eng = [doc for doc in copy.deepcopy(tokenized_documents_test).values()]\n",
    "X_full = X_train_eng + X_val_eng + X_test_eng + [tok_abstr] # add words for specific examples\n",
    "\n",
    "# Our vocab: all the words in all abstracts\n",
    "target_vocab = list(set([token for doc in X_full for token in doc]))\n",
    "# Dictionary with all words and their indices\n",
    "vocab_ind_dict = dict(zip(target_vocab, range(0, len(target_vocab)))) \n",
    "# Embedding matrix\n",
    "embed_matrix = glove_emb_matrix(vocab_ind_dict, glove, glove_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for network\n",
    "X_train = data_to_seq(X_train_eng, vocab_ind_dict)\n",
    "X_val = data_to_seq(X_val_eng, vocab_ind_dict)\n",
    "X_test = data_to_seq(X_test_eng, vocab_ind_dict)\n",
    "\n",
    "kp_train = [doc for doc in copy.deepcopy(tokenized_labels_train).values()]\n",
    "tags_train = [doc for doc in copy.deepcopy(tags_train).values()]\n",
    "kp_val = [doc for doc in copy.deepcopy(tokenized_labels_val).values()]\n",
    "tags_val = [doc for doc in copy.deepcopy(tags_val).values()]\n",
    "kp_test = [doc for doc in copy.deepcopy(tokenized_labels_test).values()]\n",
    "tags_test = [doc for doc in copy.deepcopy(tags_test).values()]\n",
    "\n",
    "# Padding \n",
    "X_train_padded = pad_sequences(X_train, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "X_val_padded = pad_sequences(X_val, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "X_test_padded = pad_sequences(X_test, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "\n",
    "tags_train_padded = pad_sequences(tags_train, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "tags_val_padded = pad_sequences(tags_val, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "tags_test_padded = pad_sequences(tags_test, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "\n",
    "# Convert labels to 3D as keras likes\n",
    "tags_train_2d = tags_to_2D(tags_train_padded)\n",
    "tags_val_2d = tags_to_2D(tags_val_padded)\n",
    "tags_test_2d = tags_to_2D(tags_test_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.reshape(class_weight.compute_sample_weight('balanced', tags_train_padded.flatten()),\n",
    "                             np.shape(tags_train_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 550, 100)          1821500   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 550, 600)          962400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 550, 600)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 550, 150)          90150     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 550, 150)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 550, 2)            302       \n",
      "=================================================================\n",
      "Total params: 2,874,352\n",
      "Trainable params: 1,052,852\n",
      "Non-trainable params: 1,821,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 993 samples, validate on 492 samples\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_SIZE = 100\n",
    "MAX_DOCUMENT_LENGTH = 550\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 0\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(np.shape(embed_matrix)[0],\n",
    "                            EMBEDDINGS_SIZE,\n",
    "                            weights=[embed_matrix],\n",
    "                            input_length=MAX_DOCUMENT_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(LSTM(300, activation='tanh', recurrent_activation='hard_sigmoid', return_sequences=True)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(TimeDistributed(Dense(150, activation='relu', kernel_regularizer=regularizers.l2(0.01))))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(TimeDistributed(Dense(2, activation='softmax')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'],\n",
    "              sample_weight_mode=\"temporal\")\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "history = model.fit(X_train_padded, tags_train_2d,\n",
    "                    validation_data=(X_val_padded, tags_val_2d),\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    sample_weight=weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(documents_eng, kp_eng, documents_seq, tags, model):\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    f_score = 0\n",
    "    acc = 0\n",
    "    predictions = model.predict_classes(documents_seq)\n",
    "    for idx, document_eng in enumerate(documents_eng):\n",
    "        # our document (unpadding)\n",
    "        doc_len = len(documents_eng[idx])\n",
    "        document_seq = documents_seq[idx][0:doc_len]\n",
    "        tags_predicted = predictions[idx][0:doc_len]\n",
    "        # predicted kp\n",
    "        kp_predicted = retrive_phrase_IO(tags_predicted, document_eng)\n",
    "        kp_true = kp_eng[idx]\n",
    "        tags_true = tags[idx]\n",
    "        # compute precision, recall, f_score, accuracy\n",
    "        prec += precision(kp_true, kp_predicted)\n",
    "        rec += recall(kp_true, kp_predicted)\n",
    "        f_score += f1(kp_true, kp_predicted)\n",
    "        acc += sum(np.equal(tags_true, tags_predicted))/len(tags_true)\n",
    "    return prec/len(documents_eng), rec/len(documents_eng), f_score/len(documents_eng), acc/len(documents_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation part ideally should be inside training to control overfitting (right way of doing this is using callback function, didn't have time to do this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 0.5982191489239921\n",
      "Validation Precision 0.02464447105112574\n",
      "Validation Recall 0.04062972913969666\n",
      "Validation F-score 0.0291223935894617\n"
     ]
    }
   ],
   "source": [
    "pr, r, f, acc = validate(X_val_eng, kp_val, X_val_padded, tags_val, model)\n",
    "print('Validation Accuracy', acc)\n",
    "print('Validation Precision', pr)\n",
    "print('Validation Recall', r)\n",
    "print('Validation F-score', f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 0.6038567115190553\n",
      "Validation Precision 0.020568483718766118\n",
      "Validation Recall 0.03396724862552323\n",
      "Validation F-score 0.02403948789915896\n"
     ]
    }
   ],
   "source": [
    "pr, r, f, acc = validate(X_test_eng, kp_test, X_test_padded, tags_test, model)\n",
    "print('Validation Accuracy', acc)\n",
    "print('Validation Precision', pr)\n",
    "print('Validation Recall', r)\n",
    "print('Validation F-score', f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time to get predictions for our tok_abstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rl'], ['seeks'], ['expressed'], ['constraints'], ['for', 'instance', ',', 'the', 'designer'], ['want'], ['exploration', ','], ['paper', ',', 'we', 'propose', 'an', 'algorithmic', 'scheme'], ['rl', 'tasks'], ['require'], ['an'], ['convex'], ['an'], ['comes', 'with', 'rigorous', 'theoretical', 'guarantees', 'and', 'only', 'relies', 'on', 'the'], ['solve', 'standard', 'rl', 'tasks'], ['any'], ['model-based', 'rl', 'algorithm', '.'], [',', 'we', 'show', 'that', 'it', 'matches', 'previous', 'algorithms', 'that', 'enforce', 'safety', 'via'], ['can', 'also']]\n"
     ]
    }
   ],
   "source": [
    "X = copy.deepcopy(tok_abstr)\n",
    "for j, token in enumerate(tok_abstr):\n",
    "    X[j] = vocab_ind_dict[token]\n",
    "\n",
    "X = pad_sequences([X], maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "predictions = model.predict_classes(X)\n",
    "\n",
    "kp = retrive_phrase_IO(predictions[0], tok_abstr)\n",
    "print(kp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
