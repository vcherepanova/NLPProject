{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from keras import regularizers\n",
    "from keras.layers import Bidirectional, Dense, Dropout, Embedding, LSTM, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "from data_preprocessing import open_data, tokenize, tag_document, data_to_seq, glove_emb_matrix, tags_to_3D, clean_data\n",
    "from validation import precision, recall, f1, retrive_phrase_BIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/valeriya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "documents = {}\n",
    "labels = {}\n",
    "\n",
    "#directories\n",
    "dir_Tu= \"/Users/kmirai/Downloads/NLPProject-master/Hulth2003/Training\"\n",
    "\n",
    "dir_valeria_train = \"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Training\"\n",
    "dir_valeria_val = \"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Validation\"\n",
    "dir_valeria_test = \"/home/valeriya/Desktop/UMD/Computational_linguistic/Project/Hulth2003/Hulth2003/Test\"\n",
    "\n",
    "dir_anna = \"/Users/annasotnikova/Downloads/Hulth2003/Training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#open data\n",
    "documents_train, labels_train = open_data(dir_valeria_train)\n",
    "documents_val, labels_val = open_data(dir_valeria_val)\n",
    "documents_test, labels_test = open_data(dir_valeria_test)\n",
    "\n",
    "# tokenize data\n",
    "tokenized_documents_train, tokenized_labels_train = tokenize(documents_train, labels_train)\n",
    "tokenized_documents_val, tokenized_labels_val = tokenize(documents_val, labels_val)\n",
    "tokenized_documents_test, tokenized_labels_test = tokenize(documents_test, labels_test)\n",
    "\n",
    "# create sequence of labels (tags) for the documents\n",
    "tags_train, tokenized_labels_train = tag_document(tokenized_documents_train, tokenized_labels_train)\n",
    "tags_val, tokenized_labels_val = tag_document(tokenized_documents_val, tokenized_labels_val)\n",
    "tags_test, tokenized_labels_test = tag_document(tokenized_documents_test, tokenized_labels_test)\n",
    "\n",
    "# remove documents without keyphrases \n",
    "tokenized_documents_train, tags_train, tokenized_labels_train = clean_data(tokenized_documents_train,\n",
    "                                                                           tags_train, tokenized_labels_train)\n",
    "tokenized_documents_val, tags_val, tokenized_labels_val = clean_data(tokenized_documents_val,\n",
    "                                                                           tags_val, tokenized_labels_val)\n",
    "tokenized_documents_test, tags_test, tokenized_labels_test = clean_data(tokenized_documents_test,\n",
    "                                                                           tags_test, tokenized_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOVE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove = dict()\n",
    "f = open('glove.6B/glove.6B.200d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove[word] = coefs\n",
    "f.close()\n",
    "\n",
    "glove_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary from all data \n",
    "X_train_eng = [doc for doc in copy.deepcopy(tokenized_documents_train).values()]\n",
    "X_val_eng = [doc for doc in copy.deepcopy(tokenized_documents_val).values()]\n",
    "X_test_eng = [doc for doc in copy.deepcopy(tokenized_documents_test).values()]\n",
    "X_full = X_train_eng + X_val_eng + X_test_eng\n",
    "\n",
    "# Our vocab: all the words in all abstracts\n",
    "target_vocab = list(set([token for doc in X_full for token in doc]))\n",
    "# Dictionary with all words and their indices\n",
    "vocab_ind_dict = dict(zip(target_vocab, range(0, len(target_vocab)))) \n",
    "# Embedding matrix\n",
    "embed_matrix = glove_emb_matrix(vocab_ind_dict, glove, glove_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for network\n",
    "X_train = data_to_seq(X_train_eng, vocab_ind_dict)\n",
    "X_val = data_to_seq(X_val_eng, vocab_ind_dict)\n",
    "X_test = data_to_seq(X_test_eng, vocab_ind_dict)\n",
    "\n",
    "kp_train = [doc for doc in copy.deepcopy(tokenized_labels_train).values()]\n",
    "tags_train = [doc for doc in copy.deepcopy(tags_train).values()]\n",
    "kp_val = [doc for doc in copy.deepcopy(tokenized_labels_val).values()]\n",
    "tags_val = [doc for doc in copy.deepcopy(tags_val).values()]\n",
    "kp_test = [doc for doc in copy.deepcopy(tokenized_labels_test).values()]\n",
    "tags_test = [doc for doc in copy.deepcopy(tags_test).values()]\n",
    "\n",
    "# Padding \n",
    "X_train_padded = pad_sequences(X_train, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "X_val_padded = pad_sequences(X_val, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "X_test_padded = pad_sequences(X_test, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "\n",
    "tags_train_padded = pad_sequences(tags_train, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "tags_val_padded = pad_sequences(tags_val, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "tags_test_padded = pad_sequences(tags_test, maxlen=550, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "\n",
    "# Convert labels to 3D as keras likes\n",
    "tags_train_3d = tags_to_3D(tags_train_padded)\n",
    "tags_val_3d = tags_to_3D(tags_val_padded)\n",
    "tags_test_3d = tags_to_3D(tags_test_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = np.reshape(class_weight.compute_sample_weight('balanced', tags_train_padded.flatten()),\n",
    "                             np.shape(tags_train_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 550, 200)          3642600   \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 550, 1000)         2804000   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 550, 1000)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 550, 150)          150150    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 550, 150)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 550, 3)            453       \n",
      "=================================================================\n",
      "Total params: 6,597,203\n",
      "Trainable params: 2,954,603\n",
      "Non-trainable params: 3,642,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 993 samples, validate on 492 samples\n",
      "Epoch 1/12\n",
      "993/993 [==============================] - 34s 34ms/step - loss: 2.0140 - accuracy: 0.7572 - val_loss: 0.9025 - val_accuracy: 0.8765\n",
      "Epoch 2/12\n",
      "993/993 [==============================] - 31s 31ms/step - loss: 0.7186 - accuracy: 0.8692 - val_loss: 0.2890 - val_accuracy: 0.9359\n",
      "Epoch 3/12\n",
      "993/993 [==============================] - 31s 32ms/step - loss: 0.4111 - accuracy: 0.8909 - val_loss: 0.4849 - val_accuracy: 0.8608\n",
      "Epoch 4/12\n",
      "993/993 [==============================] - 31s 32ms/step - loss: 0.3421 - accuracy: 0.8962 - val_loss: 0.3046 - val_accuracy: 0.9009\n",
      "Epoch 5/12\n",
      "993/993 [==============================] - 31s 32ms/step - loss: 0.3142 - accuracy: 0.9032 - val_loss: 0.4077 - val_accuracy: 0.8967\n",
      "Epoch 6/12\n",
      "993/993 [==============================] - 31s 32ms/step - loss: 0.2979 - accuracy: 0.9033 - val_loss: 0.1673 - val_accuracy: 0.9376\n",
      "Epoch 7/12\n",
      "993/993 [==============================] - 34s 34ms/step - loss: 0.2787 - accuracy: 0.9094 - val_loss: 0.2250 - val_accuracy: 0.9139\n",
      "Epoch 8/12\n",
      "993/993 [==============================] - 33s 33ms/step - loss: 0.2631 - accuracy: 0.9119 - val_loss: 0.2026 - val_accuracy: 0.9348\n",
      "Epoch 9/12\n",
      "993/993 [==============================] - 33s 33ms/step - loss: 0.2504 - accuracy: 0.9129 - val_loss: 0.2744 - val_accuracy: 0.9167\n",
      "Epoch 10/12\n",
      "993/993 [==============================] - 32s 32ms/step - loss: 0.2391 - accuracy: 0.9167 - val_loss: 0.2198 - val_accuracy: 0.9306\n",
      "Epoch 11/12\n",
      "993/993 [==============================] - 32s 32ms/step - loss: 0.2261 - accuracy: 0.9180 - val_loss: 0.3026 - val_accuracy: 0.9075\n",
      "Epoch 12/12\n",
      "993/993 [==============================] - 32s 32ms/step - loss: 0.2091 - accuracy: 0.9210 - val_loss: 0.2882 - val_accuracy: 0.9062\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_SIZE = 100\n",
    "MAX_DOCUMENT_LENGTH = 550\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 12\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(np.shape(embed_matrix)[0],\n",
    "                            EMBEDDINGS_SIZE,\n",
    "                            weights=[embed_matrix],\n",
    "                            input_length=MAX_DOCUMENT_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(LSTM(300, activation='tanh', recurrent_activation='hard_sigmoid', return_sequences=True)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(TimeDistributed(Dense(150, activation='relu', kernel_regularizer=regularizers.l2(0.01))))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(TimeDistributed(Dense(3, activation='softmax')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'],\n",
    "              sample_weight_mode=\"temporal\")\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "history = model.fit(X_train_padded, tags_train_3d,\n",
    "                    validation_data=(X_val_padded, tags_val_3d),\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    sample_weight=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 993 samples, validate on 492 samples\n",
      "Epoch 1/2\n",
      "993/993 [==============================] - 232s 234ms/step - loss: 0.2229 - accuracy: 0.9207 - val_loss: 0.1773 - val_accuracy: 0.9472\n",
      "Epoch 2/2\n",
      "993/993 [==============================] - 231s 232ms/step - loss: 0.1982 - accuracy: 0.9254 - val_loss: 0.2472 - val_accuracy: 0.9228\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded, tags_train_3d,\n",
    "                    validation_data=(X_val_padded, tags_val_3d),\n",
    "                    epochs=2,\n",
    "                    batch_size=4,\n",
    "                    sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(documents_eng, kp_eng, documents_seq, tags, model):\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    f_score = 0\n",
    "    acc = 0\n",
    "    predictions = model.predict_classes(documents_seq)\n",
    "    for idx, document_eng in enumerate(documents_eng):\n",
    "        # our document (unpadding)\n",
    "        doc_len = len(documents_eng[idx])\n",
    "        document_seq = documents_seq[idx][0:doc_len]\n",
    "        tags_predicted = predictions[idx][0:doc_len]\n",
    "        # predicted kp\n",
    "        kp_predicted = retrive_phrase_BIO(tags_predicted, document_eng)\n",
    "        kp_true = kp_eng[idx]\n",
    "        tags_true = tags[idx]\n",
    "        # compute precision, recall, f_score, accuracy\n",
    "        prec += precision(kp_true, kp_predicted)\n",
    "        rec += recall(kp_true, kp_predicted)\n",
    "        f_score += f1(kp_true, kp_predicted)\n",
    "        acc += sum(np.equal(tags_true, tags_predicted))/len(tags_true)\n",
    "        #if idx == 1:\n",
    "        #    print('document_eng', document_eng)\n",
    "        #    print('document_seq', document_seq)\n",
    "        #    print(\"kp_true\",kp_true)\n",
    "        #    print(\"tags_true\" ,tags_true)\n",
    "        #    print(\"tags_predicted\", tags_predicted)\n",
    "        #    print(\"kp_predicted\", kp_predicted)\n",
    "    return prec/len(documents_eng), rec/len(documents_eng), f_score/len(documents_eng), acc/len(documents_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 0.6837211921513269\n",
      "Validation Precision 0.21661790446645643\n",
      "Validation Recall 0.6801527987727779\n",
      "Validation F-score 0.3170095497740277\n"
     ]
    }
   ],
   "source": [
    "pr, r, f, acc = validate(X_val_eng, kp_val, X_val_padded, tags_val, model)\n",
    "print('Validation Accuracy', acc)\n",
    "print('Validation Precision', pr)\n",
    "print('Validation Recall', r)\n",
    "print('Validation F-score', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 0.6859686638602077\n",
      "Validation Precision 0.22138383494617045\n",
      "Validation Recall 0.6927075645606816\n",
      "Validation F-score 0.3242031486830308\n"
     ]
    }
   ],
   "source": [
    "pr, r, f, acc = validate(X_test_eng, kp_test, X_test_padded, tags_test, model)\n",
    "print('Validation Accuracy', acc)\n",
    "print('Validation Precision', pr)\n",
    "print('Validation Recall', r)\n",
    "print('Validation F-score', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
